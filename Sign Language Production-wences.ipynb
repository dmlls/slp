{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6abe7d",
   "metadata": {},
   "source": [
    "<img vspace=\"33px\" align=\"right\" src=\"https://www.munich-startup.de/wp-content/uploads/2019/03/TUM_logo-440x236.png\" width=\"120px\"/>\n",
    "<h1>Sign Language Production</h1>\n",
    "<h3>Applied Deep Learning for NLP</h3>\n",
    "<p><b>Diego Miguel Lozano</b> | <b>Wenceslao Villegas Marset</b></p>\n",
    "<p>March 9<sup>th</sup>, 2022</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe5979",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23da87",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "> ### [1. Introduction](#section_1)\n",
    ">> [**1.1 What is Sign Language Production (SLP)?**](#section_1_1)<br>\n",
    ">> [**1.2 What is the starting point for our project?**](#section_1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f6f093",
   "metadata": {},
   "source": [
    "# 0. Set-up and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "25786f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from torch import nn\n",
    "from torchtext import data\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, logging\n",
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f38d01",
   "metadata": {},
   "source": [
    "<a id='section_1'></a>\n",
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ab10a",
   "metadata": {},
   "source": [
    "<a id='section_1_1'></a>\n",
    "### 1.1 What is Sign Language Production (SLP)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2061012",
   "metadata": {},
   "source": [
    "Sign Language Production focuses on translating spoken languages into sign languages and viceversa. According to the World Health Organization (WHO), in 2020 there were more than 466 million deaf people in the world [[1]](#ref_1). This area could be of great help for the hearing-impared community, being for that necessary the development of techniques for both recognition and production of sign languages.\n",
    "\n",
    "While the Sign Language Recognition has seen numerous advancements in the last years [[2](#ref_2), [3](#ref_3), [4](#ref_4), [5](#ref_5), [6](#ref_6), [7](#ref_7)], Sign Language Production is still a very challenging task, since it involves an interpretation between visual and linguistic information [[8]](#ref_8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2022d6f1",
   "metadata": {},
   "source": [
    "<a id='section_1_2'></a>\n",
    "### 1.2 What is the starting point for our project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3128eb0",
   "metadata": {},
   "source": [
    "As we just mentioned, SLP is complex and far from being solved. Nevertheless, there have recently been promising developments, such as the application of Transformer architectures to SLP, what has come to be called \"Progressive Transformers.\"\n",
    "\n",
    "In this project, we take the [source code](https://github.com/BenSaunders27/ProgressiveTransformersSLP) for the paper \"Progressive Transformers for End-to-End Sign Language Production\" [[9]](#ref_9) as the starting point.\n",
    "\n",
    "We propose to test different improvement approaches to boost the model's performance like:\n",
    "\n",
    "\n",
    "* Using pre-trained BERT embeddings and fine-tuning them during training.\n",
    "* Leveraging different pre-trained models from the hugging-face ecosystem to perform data augmentation on the source senteces. \n",
    "* Testing out the improvement in performance with different hyperparameter configurations for the transformer architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a088674",
   "metadata": {},
   "source": [
    "<a id='section_1_3'></a>\n",
    "### 1.3 The data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4798e",
   "metadata": {},
   "source": [
    "Source data stems from the RWTH-PHOENIX-Weather-2014T dataset.\n",
    "\n",
    "*Dataset Infomation*: Over a period of three years (2009 - 2011) the daily news and weather forecast airings of the German public tv-station PHOENIX featuring sign language interpretation have been recorded and the weather forecasts of a subset of 386 editions have been transcribed using gloss notation. Furthermore, we used automatic speech recognition with manual cleaning to transcribe the original German speech. As such, this corpus allows to train end-to-end sign language translation systems from sign language video input to spoken language.\n",
    "\n",
    "The signing is recorded by a stationary color camera placed in front of the sign language interpreters. Interpreters wear dark clothes in front of an artificial grey background with color transition. All recorded videos are at 25 frames per second and the size of the frames is 210 by 260 pixels. Each frame shows the interpreter box only.\n",
    "\n",
    "* **Text data.**:\n",
    "\n",
    "Consists of sequences of text that represent the transcription of each video recording.\n",
    "\n",
    "Example: \n",
    "> das bedeutet viele wolken und immer wieder zum teil kr√§ftige schauer und gewitter \n",
    "\n",
    "* **Gloss representation**:\n",
    "\n",
    "Gloss equivalent for the text transcript. A gloss is a German word or words that are used to name the corresponding Sign Language signs.\n",
    "\n",
    "Example: \n",
    "> ES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
    "\n",
    "* **Skeleton**:\n",
    "\n",
    "Sequence of 3D skeletal poses that the model has to learn to produce (ground truth). The format of this data is as follows:\n",
    "\n",
    "* Tuple format: (index of a start point, index of an end point, index of a bone)\n",
    "\n",
    "                (0)\n",
    "                 |\n",
    "                 |\n",
    "                 0\n",
    "                 |\n",
    "                 |\n",
    "        (2)--1--(1)--1--(3)\n",
    "         |               |\n",
    "         |               |\n",
    "         2               2\n",
    "         |               |\n",
    "         |               |\n",
    "        (4)             (5)\n",
    "\n",
    "      has this structure:\n",
    "\n",
    "      (\n",
    "        (0, 1, 0),\n",
    "        (1, 2, 1),\n",
    "        (1, 3, 1),\n",
    "        (2, 4, 2),\n",
    "        (3, 5, 2),\n",
    "      )\n",
    "\n",
    "Then a resulting skeleton pose on a frame would be composed of 150 values, since we have 25 joints with xyz coordinates. And each text sample would have N corresponding frames that would represent the corresponding sequence of skeleton poses. During training the shape of the network output is 151 since a counter is added for the network to predict (explained further down).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4157bb9e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66347de",
   "metadata": {},
   "source": [
    "# A brief intro to the \"Progressive Transformers for SLP\" project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194c2c27",
   "metadata": {},
   "source": [
    "In this section, we will quickly explain the main aspects of Progressive Transformers project. If we had to summarize it in only three points, these would be the following: counter decoding, two different approaches ‚ÄìText-to-Gloss-to-Pose (T2G2P) and Text-to-Pose (T2P)‚Äì, and data augmentation.\n",
    "\n",
    "## Counter decoding\n",
    "\n",
    "One of the main challenges of SLP is that the output has to maintain certain continuity. The predicted pose in a video frame has to flow naturally from the previous one, and analogously for the frames that follow. This is achieved in the following manner: the model not only predicts the sign pose, but also a \"counter\". This counter is nothing else but real number in the interval [0, 1]. This value increases monotonically from 0 to 1.0, marking thus the begining and end of sequence, respectively.\n",
    "\n",
    "<img width=\"600px\" src=\"./images/counter-decoding.jpg\" alt=\"Counter Decoding\"/>\n",
    "<br>\n",
    "<div align=\"center\"><i>Representation of counter decoding.</i></div>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "**‚ÑπÔ∏è Question: Why not simply use an BOS token and EOS?**\n",
    "\n",
    "**üí° Answer:** Begining of Sentence (BOS) and End of Sentence (EOS) tokens work well with sentences, but when producing video as we mentioned before we need something more than just marking the beginning and end of it. Therefore, the counter serves both as an BOS and EOS and captures information about the flow of the video.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Two different approaches\n",
    "\n",
    "In the paper, they experimented with two different approaches: T2G2P and T2P:\n",
    "\n",
    "<img width=\"600px\" src=\"./images/T2G2P-vs-T2P.png\" alt=\"T2G2P vs T2P Architectures\"/>\n",
    "<br>\n",
    "<div align=\"center\"><i>Architecture details of (a) Symbolic and (b) Progressive Transformers. (ST: Symbolic Transformer, PT: Progressive Transformer, PE: Positional Encoding, CE: Counter Embedding, MHA: Multi-Head Attention) <a href=\"#ref_10\">[10]</a>.</i></div>\n",
    "\n",
    "In both cases, the models follow the architecture introduced in \"Attention is All You Need\" <a href=\"#ref_11\">[10]</a>.\n",
    "\n",
    "In the first approach ‚ÄìT2G2P‚Äì glosses are produced from the input tokens in a first step. Then, this glosses serve as input for another transformer, which then translates the glosses into sign poses.\n",
    "\n",
    "The second ‚ÄìT2P‚Äì is an end-to-end approach, in which the text is directly translated into sign poses.\n",
    "\n",
    "### Data augmentation\n",
    "\n",
    "Finally, the paper explores some data augmentation techniques to determine whether they improve the base model. These augmentations where only carried out with the T2G2P architecture.\n",
    "\n",
    "- **Future Prediction**: this type of augmentation forces the model to predict the next 10 frames from the current time step, instead of just the next frame. In this way, the model cannot just copy the previous time step, which effectively improves performance over the base architecture.\n",
    "\n",
    "\n",
    "- **Just Counter**: in this case only the counter values are provided as target input to the model, omitting the 3D skeleton joint coordinates. Again, this has shown to improve results.\n",
    "\n",
    "\n",
    "- **Gaussian Noise**: the last augmentation method consists in adding Gaussian noise to the skeleton pose sequences during training. This makes the model more robust to prediction inputs.\n",
    "\n",
    "\n",
    "The following table collects the results of the previous augmentation approaches:\n",
    "\n",
    "<img width=\"700px\" src=\"./images/augmentation-results.png\" alt=\"Data Augmentation Results\"/>\n",
    "<br>\n",
    "<div align=\"center\"><i>The best BLEU-4 performance comes from a combination of future prediction and Gaussian noise augmentation. The model must learn to cope with both multi-frame prediction and a noisy input, building a firm robustness to drift <a href=\"#ref_10\">[10]</a>.</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d98915",
   "metadata": {},
   "source": [
    "### Implementation description.\n",
    "\n",
    "The model architecture is based on the \"Attention is All You Need\" <a href=\"#ref_11\">[10]</a> paper. \n",
    "\n",
    "A relevant modification was introduced to adapt it and achieve good performance on the SLP task. \n",
    "* The final layer for the encoder is a Linear one with  512 + 1 units, which represent *coordinates of the output skeleton* + *counter decoding value*.\n",
    "\n",
    "\n",
    "\n",
    "### Overall project structure.\n",
    "\n",
    "Below a description of the modules present in the project after being further extended by us.\n",
    "\n",
    "```\n",
    "slp\n",
    "‚îÇ   README.md\n",
    "‚îÇ   Sign Language Production.ipynb    \n",
    "‚îî‚îÄ‚îÄ‚îÄimages\n",
    "‚îî‚îÄ‚îÄ‚îÄProgressiveTransformersSLP\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄConfigs\n",
    "‚îÇ       ‚îÇ   Base.yaml - Config file to set model, data loading/processing and training parameters.\n",
    "‚îÇ       ‚îÇ   src_vocab.txt - GLOSS vocabulary.\n",
    "‚îÇ       ‚îÇ   ...\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄData\n",
    "‚îÇ       ‚îÇ   train.text - Speech text for each training sample.\n",
    "‚îÇ       ‚îÇ   train.skels - Skeleton annotaions for each training sample.\n",
    "‚îÇ       ‚îÇ   train.gloss - Glosses for each training sample.\n",
    "‚îÇ       ‚îÇ   ...\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄexternal_metrics\n",
    "‚îÇ       ‚îÇ   mscoco_rouge.py - ROUGE-L metric implementation.\n",
    "‚îÇ       ‚îÇ   train.gloss - BLEU metric implementation like in https://github.com/mjpost/sacrebleu\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄoptim - Some implementations of optimization algorithms such as: lamb, RAdam, Ranger, etc.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄ__main__.py - Main entrypoing to run model training.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄbatch.py - Wrapper over torch batch iterator, adding masking and other attributes.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄbuilders.py - Assorted builder functions.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄconstants.py - Project wide constants.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄdata.py - Data loading utilities and main torchtext.data.Dataset class\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄdecoders.py - Transformer decoder implementation.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄdtw.py - Dynamic time warping imlementation as in https://github.com/pierre-rouanet/dtw.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄdecoders.py - Transformer decoder implementation.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄembedding.py - Embedding class implementation with support for BERT pretrained ones.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄencoders.py - Transformer encoder implementation.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄhelpers.py - Helper functions for logging, reporting, etc.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄinitialization.py - Custom NN parameter initialization functions.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄloss.py - Loss function implementations.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄmetrics.py - Performance metric functions.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄmodel.py - Main class assembling all the model's modules (decoder/encoder) and constructor functions.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄplot_videos.py - Validataion video generation for skeleton predictions.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄprediction.py - Code for running validation steps on dev set data (perform dtw then loss, etc).\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄsearch.py - Greedy hyperparameter search function.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄtraining.py - Training loop implementation.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄtransformer_layers.py - Layer implementations from NMT toolkit.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄvocabulary.py - Vocabulary loading and checking code.\n",
    "‚îî‚îÄ‚îÄ‚îÄaugmentations\n",
    "    ‚îÇ   backtranslation.py - Code to perform DE -> EN -> DE translation for augmentation purposes.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbd81f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d79eb",
   "metadata": {},
   "source": [
    "#  Using pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc2522",
   "metadata": {},
   "source": [
    "The original project trains embeddings from scratch. As we have learned during the seminar, the use of pretrained embeddings can effectively improve the performance of models, especially in situations where data is scarce (which is our case).\n",
    "\n",
    "Let's first see how the embedding initialization is happening in the original source code.\n",
    "\n",
    "The first thing that happens when beginning the training is the data loading. The vocabulary of the model is initalized differently depending if we are using T2G2P or T2P.\n",
    "\n",
    "- **Bulding vocabulary in T2G2P**: in this case, the vocabulary is taken from a file [`src_vocab`](https://github.com/BenSaunders27/ProgressiveTransformersSLP/blob/master/Configs/src_vocab.txt) that we will analyze a bit more in depth later.\n",
    "\n",
    "\n",
    "- **Bulding vocabulary in T2P**: when using the End-to-End approach (Text-to-Pose), the vocabulary is built from the training input data.\n",
    "\n",
    "<br>\n",
    "\n",
    "In both cases, the function [`build_vocab()`](https://github.com/BenSaunders27/ProgressiveTransformersSLP/blob/adbd3e9ea9f1b20063d84021a0d6eb9a124ebb87/vocabulary.py#L130-L187) is used. Let's take a look at it.\n",
    "\n",
    "\n",
    "```python\n",
    "if vocab_file is not None:\n",
    "    # load it from file\n",
    "    vocab = Vocabulary(file=vocab_file)\n",
    "else:\n",
    "    ...\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "First of all, if we have a vocabulary file (like in the case of T2G2P), we initialize the vocabulary from it, as we already mentioned.\n",
    "\n",
    "\n",
    "```python\n",
    "def _from_file(self, file: str) -> None:\n",
    "        \"\"\"\n",
    "        Make vocabulary from contents of file.\n",
    "        File format: token with index i is in line i.\n",
    "\n",
    "        :param file: path to file where the vocabulary is loaded from\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        with open(file, \"r\") as open_file:\n",
    "            for line in open_file:\n",
    "                tokens.append(line.strip(\"\\n\"))\n",
    "        self._from_list(tokens)\n",
    "```\n",
    "\n",
    "This function simply reads the vocabulary file line by line. Since each line contains only one token, there is no further processing to be done.\n",
    "\n",
    "If there is no input vocabulary file, the tokens are extracted from the training dataset.\n",
    "\n",
    "Let's run some code to better visualize this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52996d50",
   "metadata": {},
   "source": [
    "## Inside the original SLP model vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c610d",
   "metadata": {},
   "source": [
    "As we have already mentioned, the original project that we use as starting point provides a plain-text file [`src_vocab`](https://github.com/BenSaunders27/ProgressiveTransformersSLP/blob/master/Configs/src_vocab.txt) containing the vocabulary for which embeddings will then be trained.\n",
    "\n",
    "Before jumping in and trying to directly use our pretrained embeddings, it is sensible to first analyze a bit how things work in the original project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "733518ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = Path(\"./ProgressiveTransformersSLP\")\n",
    "\n",
    "if str(code_dir.resolve()) not in sys.path:\n",
    "    sys.path.insert(0, str(code_dir.resolve()))  # just so that imports can be resolved\n",
    "\n",
    "from ProgressiveTransformersSLP.vocabulary import Vocabulary, build_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc2f76d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>',\n",
      " '<pad>',\n",
      " '<s>',\n",
      " '</s>',\n",
      " 'HEISS',\n",
      " 'AUSWAEHLEN',\n",
      " 'BALD',\n",
      " 'BEKOMMEN',\n",
      " 'BITTE',\n",
      " 'BODENSEE',\n",
      " 'BRITANNIEN',\n",
      " 'CHAOS',\n",
      " 'DAMEN',\n",
      " 'DAUERND',\n",
      " 'DUENN',\n",
      " 'J+L+I',\n",
      " 'K+R+E+T+A',\n",
      " 'neg-EINFLUSS',\n",
      " 'neg-FUEHLEN',\n",
      " 'neg-GEWITTER',\n",
      " 'neg-HOEHE',\n",
      " 'neg-IMMER',\n",
      " 'neg-KOMMEN',\n",
      " 'neg-MEHR',\n",
      " 'negalp-BRAUCHEN',\n",
      " 'negalp-GIBT',\n",
      " 'negalp-MUSS']\n"
     ]
    }
   ],
   "source": [
    "# Path to the vocabulary file\n",
    "vocab_file = code_dir/Path(\"Configs/src_vocab.txt\")\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary = Vocabulary(file=vocab_file)\n",
    "\n",
    "# Get all the tokens in the built vocabulary\n",
    "tokens = [token for token in vocabulary.itos]\n",
    "\n",
    "# We will select some tokens that are worth analyzing\n",
    "selected_tokens = (tokens[1:5] + [tokens[172]] + tokens[531:541] +\n",
    "                   tokens[868:870] + tokens[718:725] + tokens[1085:1088])\n",
    "pprint(selected_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01866a0b",
   "metadata": {},
   "source": [
    "From the previous vocabulary, there are three aspects that are worth mentioning:\n",
    "\n",
    "- Words such as `AUSWAEHLEN`, `DUENN` and `HEISS` give us a hint that **normalization** is used. A popular algorithm for German normalization is the [German2 snowball algorithm](https://snowballstem.org/algorithms/german2/stemmer.html) which defines the following mappings:\n",
    "  - '√ü' is replaced by 'ss'.\n",
    "  - '√§', '√∂', '√º' are replaced by 'a', 'o', 'u', respectively.\n",
    "  - 'ae' and 'oe' are replaced by 'a', and 'o', respectively.\n",
    "  - 'ue' is replaced by 'u', when not following a vowel or q.\n",
    "\n",
    "\n",
    "- As we saw during the seminar lectures, the **special tokens** `<unk>`, `<pad>`, `<s>`, `</s>` are also included in the dictionary. These tokens mark unknown words, padding, beginning of sequence (BOS), and end of sequence (EOS), respectively.\n",
    "\n",
    "\n",
    "- Some of the words in the vocabulary include the prefixes `neg-` and `negalp-`. We could guess that `neg-` simply means that the word is negated, e.g., `neg-GENUG`‚â° `NICHT GENUG`, but what about the `negalp-` prefix? And also, what do words such as `J+L+I` and `K+R+E+T+A` mean? A look to the paper of the RWTH-PHOENIX-Weather dataset [[10]](#ref_10) (the first version of the dataset used to train the model) gives us the answer:\n",
    "\n",
    "\n",
    "<img width=\"400px\" src=\"./images/RWTH-PHOENIX-Weather-Annotation-Scheme.png\" alt=\"RWTH-PHOENIX-Weather Annotation Scheme\"/>\n",
    "<br>\n",
    "<div align=\"center\"><i>Source <a href=\"#ref_10\">[10]</a>.</i></div>\n",
    "\n",
    "So in reality `neg-` means \"signs negated by headshake\" and `negalp-` \"signs negated by the alpha[betical] rule\" <sup>[1](note_1)</sup>. Words such as `K+R+E+T+A` are words (finger) spelled letter by letter.\n",
    "\n",
    "Interestingly enough, none of the other types of tokens appear in the source dictionary.\n",
    "\n",
    "---\n",
    "\n",
    "<a id='note_1'><sup>1</sup></a> In Sign Language, there are several ways of negating words. One of these ways is using a side-to-side headshake or a frown expression. Also, some verbs have their own negated forms, which is what `negalp-` indicates here [[11]](#ref_11)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51200a59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**‚ÑπÔ∏è Question: But... where is this vocabulary coming from?**\n",
    "\n",
    "**üí° Answer:** As it turns out, this vocabulary is simply made up of **glosses**. As we mentioned before, the original project proposes two ways of carrying out the translations from text to sign language. That also explains why when going for the T2P approach we don't use this file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c0e3d",
   "metadata": {},
   "source": [
    "## BERT it up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa8f4e",
   "metadata": {},
   "source": [
    "If something is clear is that BERT's dictionary will not contain glosses, let alone glosses specifically tailored to SLP.\n",
    "\n",
    "But then, how does BERT's vocabulary looks like? Let's take a look!\n",
    "\n",
    "Fortunately, Hugging Face's great API has got us covered: tokenizers expose their vocabulary through the method `get_vocab()`. Let's try with the model [`bert-base-german-cased`](https://huggingface.co/bert-base-german-cased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5008fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dismiss expected \"Some weights of the model checkpoint at...\" warning\n",
    "# when loading a pretrained model.\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Load tokenizer and model from pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"dbmdz/bert-base-german-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80f95195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('elektrotechnik', 27915),\n",
      " ('tr√§gt', 4924),\n",
      " ('lauf', 3699),\n",
      " ('weihn', 3862),\n",
      " ('##:29', 7501),\n",
      " ('##ausgleich', 17336),\n",
      " ('schwand', 24253),\n",
      " ('##√∂hnen', 17915),\n",
      " ('straub', 27572),\n",
      " ('##dienste', 5320),\n",
      " ('lic', 23955),\n",
      " ('kurven', 22262),\n",
      " ('##02.', 6984),\n",
      " ('##musik', 4110),\n",
      " ('theo', 27146),\n",
      " ('letztes', 11053),\n",
      " ('russischer', 19267),\n",
      " ('schmie', 29874),\n",
      " ('##politische', 14619),\n",
      " ('5000', 12509),\n",
      " ('wohnzimmer', 10453),\n",
      " ('##≈ç', 31039),\n",
      " ('solution', 24133),\n",
      " ('##arbeitete', 10860),\n",
      " ('herauszu', 14287),\n",
      " ('##mente', 15954),\n",
      " ('##elementen', 24297),\n",
      " ('##zur', 5215),\n",
      " ('emir', 18817),\n",
      " ('##fast', 4720),\n",
      " ('##ktisch', 15657),\n",
      " ('##√ºn', 311),\n",
      " ('liebte', 28487),\n",
      " ('bergl', 18508),\n",
      " ('1828', 27261),\n",
      " ('##vot', 26826),\n",
      " ('unused94', 95),\n",
      " ('38.', 26319),\n",
      " ('zertifizierung', 27349),\n",
      " ('ital', 21301)]\n"
     ]
    }
   ],
   "source": [
    "# Print some words of its dictionary\n",
    "pprint(list(tokenizer.get_vocab().items())[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b49e6b",
   "metadata": {},
   "source": [
    "Above, we can see each token with its corresponding ID (just as we saw in the seminar lectures). However, there are two things that catch our attention...\n",
    "\n",
    "---\n",
    "\n",
    "**‚ÑπÔ∏è Question: Why do some tokens start with \"##\"?**\n",
    "\n",
    "**üí° Answer:** Well, this is just a way of indicating that this token is \"non-initial\", i.e., originally it belonged to a longer word. This way of tokenizing words comes from the [WordPiece](https://paperswithcode.com/method/wordpiece) algorithm, which is used by the BERT tokenizer.\n",
    "\n",
    "---\n",
    "\n",
    "**‚ÑπÔ∏è Question: What about the `'unused...'` tokens?**\n",
    "\n",
    "**üí° Answer:** These are, unsurprisingly, tokens that are not used. However, they can come handy to add more words to the vocabulary:\n",
    "\n",
    "> Just replace the \"[unusedX]\" tokens with your vocabulary. Since these were not used they are effectively randomly initialized. ([source](https://github.com/google-research/bert/issues/9#issuecomment-434796704))\n",
    "\n",
    "---\n",
    "\n",
    "As a side note, we would like to mention that someone took the time to explore BERT's vocabulary and wrote a great article about it. The article can be found at https://juditacs.github.io/2019/02/19/bert-tokenization-stats.html.\n",
    "\n",
    "---\n",
    "\n",
    "We might also want to take a look at the emeddings of a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1474001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0589,  0.0175,  0.0557, -0.0638,  0.0209, -0.0444, -0.0280,  0.0746,\n",
      "        -0.0919,  0.0341, -0.0629,  0.0872,  0.0065,  0.0207,  0.0331, -0.0571,\n",
      "        -0.0225, -0.0220,  0.0595,  0.0685, -0.1042,  0.0522, -0.0253,  0.0563,\n",
      "        -0.0017, -0.0496,  0.0112, -0.0188, -0.0332, -0.0097, -0.0394, -0.0409],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Dimensions: 768\n"
     ]
    }
   ],
   "source": [
    "# Get token ID (lowercase since the model is uncased\n",
    "token_id = tokenizer.get_vocab()[\"Frieden\".lower()]\n",
    "embeddings = model.get_input_embeddings()(torch.tensor(token_id))\n",
    "\n",
    "# Print only the 32 first values\n",
    "print(embeddings[:32])\n",
    "print(\"\\nDimensions:\", len(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14767d4f",
   "metadata": {},
   "source": [
    "### Alright, so how do we plug in the pretrained embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2ec69",
   "metadata": {},
   "source": [
    "In order to use the pretrained embeddings, we had to carry out several modifications.\n",
    "\n",
    "The **original codebase** is **complex** and there are many intertwined components. One of our main goals was to add functionality in a way that what was there before could still be used. This means that we had to make these changes in a way that the baseline settings could still be used.\n",
    "\n",
    "At first we try to clean up and refactor the code, but we soon realized that, due to the size of the original project, it would take too long and deliver no value to our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98be523",
   "metadata": {},
   "source": [
    "## Pretrained embeddings: list of changes to be made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c6c05",
   "metadata": {},
   "source": [
    "*Note: all the files listed below are located in the `ProgressiveTransformersSLP` directory.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274e6f8",
   "metadata": {},
   "source": [
    "### Adding a configuration parameter in `Configs/Base.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b0411",
   "metadata": {},
   "source": [
    "`Configs/Base.yaml` is the file that defines the different configuration parameters for the **dataset, model and training**. Here we can find, for example, training hyperparameters such as the learning rate or the number of epochs, but also the embedding dimensions of our model, etc.\n",
    "\n",
    "It looks like follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e95a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "    src: \"text\" # Source - Either Gloss->Pose or Text->Pose (gloss,text)\n",
      "    trg: \"skels\" # Target - 3D body co-ordinates (skels)\n",
      "    files: \"files\" # Filenames for each sequence\n",
      "\n",
      "    train: \"./slp/ProgressiveTransformersSLP/Data/tmp/train\"\n",
      "    dev: \"./slp/ProgressiveTransformersSLP/Data/tmp/dev\"\n",
      "    test: \"./slp/ProgressiveTransformersSLP/Data/tmp/test\"\n",
      "\n",
      "    max_sent_length: 300 # Max Sentence Length\n",
      "    skip_frames: 1 # Skip frames in the data, to reduce the data input size\n",
      "    # src_vocab: \"./Configs/src_vocab.txt\" # Gloss vocab. Only use when src: \"gloss\".\n",
      "\n",
      "training:\n",
      "    random_seed: 27 # Random seed for initialisation\n",
      "    optimizer: \"adam\" # Chosen optimiser (adam, ..)\n",
      "    learning_rate: 0.001 # Initial model learning rate\n",
      "    learning_rate_min: 0.0002 # Learning rate minimum, when training will stop\n",
      "    weight_decay: 0.0 # Weight Decay\n",
      "    clip_grad_norm: 5.0 # Gradient clipping value\n",
      "    batch_size: 8 # Batch Size for training\n",
      "    scheduling: \"plateau\" # Scheduling at training time (plateau, ...)\n",
      "    patience: 7 # How many epochs of no improvement causes a LR reduction\n",
      "    decrease_factor: 0.7 # LR reduction factor, after the # of patience epochs\n",
      "    early_stopping_metric: \"dtw\" # Which metric determines scheduling (DTW, loss, BT...)\n",
      "    epochs: 20000 # How many epochs to run for\n",
      "    validation_freq: 10 # After how many steps to run a validation on the model\n",
      "    logging_freq: 250 # After how many steps to log training progress\n",
      "    eval_metric: \"dtw\" # Evaluation metric during training (dtw','bt')\n",
      "    model_dir: \"./Models/Base\" # Where the model shall be stored\n",
      "    overwrite: False # Flag to overwrite a previous saved model in the model_dir\n",
      "    continue: True # Flag to continue from a previous saved model in the model_dir\n",
      "    shuffle: True # Flag to shuffle the data during training\n",
      "    use_cuda: True # Flag to use GPU cuda capabilities\n",
      "    max_output_length: 300 # Max Output Length\n",
      "    keep_last_ckpts: 1 # How many previous best/latest checkpoints to keep\n",
      "    loss: \"MSE\" # Loss function (MSE, L1)\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\" # Model initialisation (Xavier, ...)\n",
      "    bias_initializer: \"zeros\" # Bias initialiser (Zeros, ...)\n",
      "    embed_initializer: \"xavier\" # Embedding initialiser (Xavier, ...)\n",
      "    trg_size: 150 # Size of target skeleton coordinates (150 for Inverse Kinematics body/hands)\n",
      "    just_count_in: False # Flag for Just Counter Data Augmentation\n",
      "    gaussian_noise: False # Flag for Gaussian Noise Data Augmentation\n",
      "    noise_rate: 5 # Gaussian Noise rate\n",
      "    future_prediction: 0 # Future Prediction Data Augmentation if > 0\n",
      "    encoder: # Model Encoder\n",
      "        type: \"transformer\"\n",
      "        num_layers: 2 # Number of layers\n",
      "        num_heads: 4 # Number of Heads\n",
      "        embeddings:\n",
      "            model: \"bert\"  # Pretrained embeddings: (\"none\", \"bert\")\n",
      "            embedding_dim: 512 # Embedding Dimension (set automatically if using pretrained embeddings)\n",
      "        hidden_size: 512 # Hidden Size Dimension (set automatically if using pretrained embeddings)\n",
      "        ff_size: 2048 # Feed-forward dimension (4 x hidden_size)\n",
      "        dropout: 0.0 # Encoder Dropout\n",
      "    decoder: # Model Decoder\n",
      "        type: \"transformer\"\n",
      "        num_layers: 2 # Number of layers\n",
      "        num_heads: 4 # Number of Heads\n",
      "        embeddings:\n",
      "            embedding_dim: 512 # Embedding Dimension (set automatically if using pretrained embeddings)\n",
      "            dropout: 0.0 # Embedding Dropout\n",
      "        hidden_size: 512 # Hidden Size Dimension (set automatically if using pretrained embeddings)\n",
      "        ff_size: 2048 # Feed-forward dimension (4 x hidden_size)\n",
      "        dropout: 0.0 # Decoder Dropout\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(Path(\"./ProgressiveTransformersSLP/Configs/Base.yaml\")) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf1f4f",
   "metadata": {},
   "source": [
    "If we take a look at `model` ‚Üí `encoder`‚Üí `embeddings`, we have added a **new field** `model` that can take the values `\"none\"` (baseline model, training embeddings from scratch), or `\"bert\"`, using BERT's WordPiece embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd34ea",
   "metadata": {},
   "source": [
    "### Specifying constants in `constants.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dcce20",
   "metadata": {},
   "source": [
    "In order to correctly initialize constant values (e.g., special tokens such as 'PAD' or 'UNK') and still preserve the functionality of the original project, we had to write an admittedly \"hacky\" script that nevertheless works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb1d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Declare variables\n",
    "pretrained_model_str = None  # \"bert\" or \"none\"\n",
    "tokenizer = None   # transformers.AutoModelForMaskedLM\n",
    "vocab = None  # BERT vocabulary\n",
    "UNK_TOKEN = None\n",
    "PAD_TOKEN = None\n",
    "BOS_TOKEN = None\n",
    "EOS_TOKEN = None\n",
    "TARGET_PAD = None\n",
    "DEFAULT_UNK_ID = None\n",
    "\n",
    "special_tokens = {\n",
    "    \"none\": ('<unk>', '<pad>', '<s>', '</s>'),\n",
    "    \"bert\": ('[UNK]', '[PAD]', '[CLS]', '[SEP]')\n",
    "}\n",
    "\n",
    "\n",
    "def initialize_constants(cfg: dict):\n",
    "    global pretrained_model_str, tokenizer, vocab, UNK_TOKEN, PAD_TOKEN, \\\n",
    "        BOS_TOKEN, EOS_TOKEN, TARGET_PAD, DEFAULT_UNK_ID\n",
    "    pretrained_model_str = cfg[\"model\"][\"encoder\"][\"embeddings\"][\"model\"]\n",
    "    if pretrained_model_str not in (\"none\", \"bert\"):\n",
    "        raise ValueError(f\"embeddings from model {pretrained_model_str} not supported\")\n",
    "    UNK_TOKEN, PAD_TOKEN, BOS_TOKEN, EOS_TOKEN = special_tokens[pretrained_model_str]\n",
    "    if pretrained_model_str == \"bert\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-uncased\")\n",
    "        unk_token_id = tokenizer.get_vocab()[UNK_TOKEN]\n",
    "        # Get vocabulary, sorted by the token ids\n",
    "        vocab = [token for token in sorted(tokenizer.get_vocab().items(),\n",
    "                                           key=lambda x: x[1])]\n",
    "    else:\n",
    "        tokenizer = None\n",
    "        unk_token_id = 0\n",
    "    TARGET_PAD = 0.0\n",
    "    DEFAULT_UNK_ID = lambda: unk_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786347f3",
   "metadata": {},
   "source": [
    "First, we declare the constant variables in order to be able to import them from other files with e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf23e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK token: None\n",
      "PAD token: None\n",
      "BOS token: None\n",
      "EOS token: None\n"
     ]
    }
   ],
   "source": [
    "import ProgressiveTransformersSLP.constants as constants\n",
    "\n",
    "print(\"UNK token:\", constants.UNK_TOKEN)\n",
    "print(\"PAD token:\", constants.PAD_TOKEN)\n",
    "print(\"BOS token:\", constants.BOS_TOKEN)\n",
    "print(\"EOS token:\", constants.EOS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ba97e",
   "metadata": {},
   "source": [
    "Then, we need to call the function `initialize_constants` to initialize these constants properly, depending on whether we are using BERT embeddings or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "83062bd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens when using no pretrained embeddings:\n",
      "UNK token: <unk>\n",
      "PAD token: <pad>\n",
      "BOS token: <s>\n",
      "EOS token: </s>\n",
      "\n",
      "Special tokens when using BERT embeddings:\n",
      "UNK token: [UNK]\n",
      "PAD token: [PAD]\n",
      "BOS token: [CLS]\n",
      "EOS token: [SEP]\n"
     ]
    }
   ],
   "source": [
    "model = \"none\"\n",
    "constants.initialize_constants(cfg={\"model\": {\"encoder\": {\"embeddings\": {\"model\": model}}}})\n",
    "\n",
    "print(\"Special tokens when using no pretrained embeddings:\")\n",
    "print(\"UNK token:\", constants.UNK_TOKEN)\n",
    "print(\"PAD token:\", constants.PAD_TOKEN)\n",
    "print(\"BOS token:\", constants.BOS_TOKEN)\n",
    "print(\"EOS token:\", constants.EOS_TOKEN)\n",
    "\n",
    "\n",
    "model = \"bert\"\n",
    "constants.initialize_constants(cfg={\"model\": {\"encoder\": {\"embeddings\": {\"model\": model}}}})\n",
    "\n",
    "print(\"\\nSpecial tokens when using BERT embeddings:\")\n",
    "print(\"UNK token:\", constants.UNK_TOKEN)\n",
    "print(\"PAD token:\", constants.PAD_TOKEN)\n",
    "print(\"BOS token:\", constants.BOS_TOKEN)\n",
    "print(\"EOS token:\", constants.EOS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8740bd4",
   "metadata": {},
   "source": [
    "Executing the previous cell might trigger the download of a Hugging Face model (if it's not already cached). That's why we also initialize the `tokenizer` in case we are using BERT embeddings:\n",
    "\n",
    "```python\n",
    "    if pretrained_model_str == \"bert\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-uncased\")\n",
    "```\n",
    "\n",
    "This **avoids instanciating the tokenizer multiple times**, since we will need it in different places of our code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c941be6",
   "metadata": {},
   "source": [
    "> **üìù Note:** Another aspect that we would like to comment is that we had to choose the `[CLS]` token as the BOS (Beginning of Sequence) token and the `[SEP]` token as EOS. The meaning of these tokens goes a bit beyond what BOS and EOS signify, but they are also not too far off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd954d99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**‚ÑπÔ∏è Question: What do the `[CLS]` and `[SEP]` tokens mean in BERT?**\n",
    "\n",
    "**üí° Answer:** Both these tokens have their roots in the way BERT was trained. If we remember from the seminar, BERT was trained on two tasks: Next Sentence Classification and Maked-Language Modeling. For the second of these tasks, the token `[MASK]` was used, but we don't really need it in our case. The `[CLS]` token comes always at the beginging of sequences and is meant to hold the meaning of the whole sentence. The `[SEP]` token acts as a separator when performing Next Sentence Classification, in order to distinguish the first and second sentences <a href=\"#ref_12\">[12]</a>.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d35c5",
   "metadata": {},
   "source": [
    "### Loading BERT's vocabulary and pretrained embeddings in `vocabulary.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea55cb",
   "metadata": {},
   "source": [
    "Now that we have sorted out the correct definition of special tokens, it's time to load BERT's vocabulary and pretrained embeddings.\n",
    "\n",
    "We have already showed how to do that before, but let's recap.\n",
    "\n",
    "First, we need instances of both `tokenizer` and `model`. We have already instanciated the tokenizer in when initalizing the constants, so we only need to take care of the model. Something that we also do when this initialization happen is **load the vocabulary**. Again, we do it then since we will use this vocabulary in several places of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "66fc04a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = constants.tokenizer\n",
    "\n",
    "# Load the vocabulary, sorting by ascending token id\n",
    "vocab = [token for token in sorted(tokenizer.get_vocab().items(), key=lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7a233",
   "metadata": {},
   "source": [
    "Something very important is to **keep the order of the tokens** based on their id so that the positions of the tokens and their embeddings match. That is why we use `sorted` with `key=lambda x: x[1]`. Now, to get the embeddings, we run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "21d390e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "embed_model = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"dbmdz/bert-base-german-uncased\"\n",
    ")\n",
    "# Get the embeddings\n",
    "embeddings = torch.stack([\n",
    "    embed_model.get_input_embeddings()(torch.tensor(token[1]))\n",
    "    for token in constants.vocab\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "350c61dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0173, -0.0328, -0.0282, -0.0850, -0.0297, -0.0327, -0.0585,  0.0445,\n",
       "         0.0197, -0.0125, -0.0108,  0.0288,  0.0280, -0.0394, -0.0669,  0.0133,\n",
       "        -0.0620, -0.0023, -0.0277,  0.0127, -0.0095, -0.0883, -0.0386, -0.0213,\n",
       "        -0.0116, -0.0738, -0.0120,  0.0074, -0.0370, -0.0299],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the 30 first dimensions of the first embedding\n",
    "embeddings[0][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef81e9",
   "metadata": {},
   "source": [
    "We use `torch.stack` to get the embeddings in a flat tensor, just as if it was a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7774b79a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31102, 768])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806de0ab",
   "metadata": {},
   "source": [
    "We can see that our vocabulary contains 31102 tokens, each of them with a corresponding embedding vector of 768 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d10c89",
   "metadata": {},
   "source": [
    "Next, the tokens are added to a `Vocabulary` class through its method `add_tokens`:\n",
    "\n",
    "```python\n",
    "class Vocabulary:\n",
    "    ...\n",
    "    \n",
    "    def add_tokens(self, tokens: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Add list of tokens to vocabulary\n",
    "\n",
    "        :param tokens: list of tokens to add to the vocabulary\n",
    "        \"\"\"\n",
    "        for t in tokens:\n",
    "            new_index = len(self.itos)\n",
    "            # add to vocab if not already there\n",
    "            if t not in self.itos:\n",
    "                self.itos.append(t)\n",
    "                self.stoi[t] = new_index\n",
    "    ...\n",
    "```\n",
    "\n",
    "The attribute `self.itos` stores the tokens (strings), and `self.stoi` serves as a look-up dictionary so that, e.g., `self.itos[13]` will give us the token with id 13.\n",
    "\n",
    "\n",
    "As a side remark, the class `Vocabulary` was already implemented in the original project. We just adjusted it to be able to pass it our tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560d957",
   "metadata": {},
   "source": [
    "### Initializing the embedding layer in `embeddings.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4ca3e",
   "metadata": {},
   "source": [
    "The original project implements a torch `nn.Module` called `Embeddings` that stores the embedding weights. Internally, this module contains a unique [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer.\n",
    "\n",
    "Originally, this module was initialized like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "07243435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(31102, 768, padding_idx=0)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = embeddings.size()[1]\n",
    "padding_idx = tokenizer.get_vocab()[\"[PAD]\"]\n",
    "\n",
    "embed_layer = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "print(embed_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c0fb8",
   "metadata": {},
   "source": [
    "We can take a look at the created weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e8860534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2951, -0.5569,  1.6718,  ...,  0.0194,  1.1920, -1.4272],\n",
      "        [ 0.7343,  0.2580, -0.9960,  ...,  1.2261,  0.6621,  0.3058],\n",
      "        ...,\n",
      "        [-0.2210, -1.4156,  0.6138,  ...,  0.3274, -0.2015, -1.1469],\n",
      "        [-0.7902, -1.5453, -0.9285,  ..., -0.9304, -1.2536, -0.0898],\n",
      "        [-2.6411,  1.9783,  0.0937,  ..., -0.8000,  0.6434,  0.3154]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Size: torch.Size([31102, 768])\n"
     ]
    }
   ],
   "source": [
    "print(embed_layer.weight)\n",
    "print()\n",
    "print(\"Size:\", embed_layer.weight.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124e9fd",
   "metadata": {},
   "source": [
    "As we can see, we have a 31102 embeddings of dimension 768. This embeddings, however, are initialized randomly (sampled from $N(0,1)$).\n",
    "\n",
    "This layer can also be initalized with pretrained embeddings, which is just what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "960aaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embed = embeddings\n",
    "\n",
    "pretrained_embed_layer = nn.Embedding.from_pretrained(\n",
    "    pretrained_embed, padding_idx=padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e31078eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.7323e-02, -3.2809e-02, -2.8226e-02,  ...,  2.1750e-02,\n",
      "         -8.7762e-05,  2.9073e-02],\n",
      "        [-2.7637e-03, -6.4053e-02, -1.3206e-02,  ...,  1.7858e-02,\n",
      "          6.6482e-03, -3.0267e-02],\n",
      "        [ 1.3819e-02, -9.6506e-02,  8.0860e-04,  ...,  3.8678e-02,\n",
      "          6.3529e-02, -5.4168e-02],\n",
      "        ...,\n",
      "        [ 1.6831e-02, -5.7545e-02,  1.3673e-02,  ...,  3.3795e-02,\n",
      "          5.2264e-04, -3.5393e-02],\n",
      "        [-3.9432e-03, -3.7646e-02, -3.6883e-02,  ..., -2.8921e-02,\n",
      "          6.6119e-03, -3.0646e-02],\n",
      "        [ 4.1762e-02, -2.6133e-02, -2.7677e-02,  ...,  2.4914e-02,\n",
      "          3.1606e-02, -2.7268e-02]])\n",
      "\n",
      "Size: torch.Size([31102, 768])\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_embed_layer.weight)\n",
    "print()\n",
    "print(\"Size:\", pretrained_embed_layer.weight.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393569ca",
   "metadata": {},
   "source": [
    "Just to check, let's see if the first embedding in the newly created layer coincides with the pretrained embeddings that we loaded previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4a4a4c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great, they are equal!\n"
     ]
    }
   ],
   "source": [
    "eq = torch.equal(\n",
    "    pretrained_embed_layer.weight[0],  # embedding layer\n",
    "    embeddings[0]  # previously loaded embeddings\n",
    ")\n",
    "\n",
    "print(\"Great, they are equal!\" if eq else \"Nope, they're not equal :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b294ea4",
   "metadata": {},
   "source": [
    "### When everything seemed to go great... Houston we have a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d6e46",
   "metadata": {},
   "source": [
    "At this point, we thought that our model was ready to be trained. So we did so and got **disastrous results**. \"Why?\" ‚Äì we asked ourselves while we scratched our heads.\n",
    "\n",
    "Then we realized...\n",
    "\n",
    "Let's take a look at the input training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "858178c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['und nun die wettervorhersage f√ºr morgen donnerstag den zw√∂lften august\\n',\n",
      " 'mancherorts regnet es auch l√§nger und ergiebig auch lokale √ºberschwemmungen '\n",
      " 'sind wieder m√∂glich\\n',\n",
      " 'im nordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise '\n",
      " 'kr√§ftigen schauern gerechnet werden √∂rtlich mit blitz und donner\\n',\n",
      " 'auch am tag gibt es verbreitet zum teil kr√§ftige schauer oder gewitter und '\n",
      " 'in manchen regionen fallen ergiebige regenmengen\\n',\n",
      " 'gr√∂√üere wolkenl√ºcken finden sich vor allem im nordwesten\\n',\n",
      " 'im emsland heute nacht nur neun am oberrhein bis siebzehn grad\\n',\n",
      " 'morgen √§hnliche temperaturen wie heute neunzehn bis f√ºnfundzwanzig in der '\n",
      " 'lausitz bis siebenundzwanzig grad\\n',\n",
      " 'am freitag kann es in der osth√§lfte teilweise l√§nger und kr√§ftig regnen '\n",
      " 'vorsicht hochwassergefahr\\n',\n",
      " 'sonst wechselhaft mit schauern und gewittern die uns auch am wochenende '\n",
      " 'begleiten\\n',\n",
      " 'am temperaturniveau √§ndert sich wenig\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(Path(\"ProgressiveTransformersSLP/Data/train.text\")) as f:\n",
    "    pprint(f.readlines()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa06fa90",
   "metadata": {},
   "source": [
    "Achso! The data is all **lowercased**. At that moment, we were using the model [`bert-base-german-cased`](https://huggingface.co/bert-base-german-cased). Yes, a cased model. Therefore, it was missing basically every other word (i.e., all the nouns), since they weren't in their vocabulary.\n",
    "\n",
    "We can make a quick test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1c57ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "11654aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found!\n"
     ]
    }
   ],
   "source": [
    "# Let's use the word from the previous example\n",
    "word = \"frieden\"\n",
    "\n",
    "if word in tokenizer.get_vocab():\n",
    "    print(\"Found!\")\n",
    "else:\n",
    "    print(\"Not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fce87a",
   "metadata": {},
   "source": [
    "However, if we use its cased form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "798f6afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found!\n"
     ]
    }
   ],
   "source": [
    "word = \"Frieden\"\n",
    "\n",
    "if word in tokenizer.get_vocab():\n",
    "    print(\"Found!\")\n",
    "else:\n",
    "    print(\"Not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691daaee",
   "metadata": {},
   "source": [
    "### So we changed to an uncased model... But..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db499b5",
   "metadata": {},
   "source": [
    "We trained again. And again things were not going well... So it was time to debug.\n",
    "\n",
    "The **data loading and handling** is somehow **complex** (excessively perhaps?). It took us quite some time to realize where things were going wrong.\n",
    "\n",
    "In summary, how data is managed is as follows:\n",
    "\n",
    "1. First, the data is read from the input files (stored at `ProgressiveTransformersSLP/Data`). This is done via the function [`load_data`](https://github.com/dmlls/slp/blob/main/ProgressiveTransformersSLP/data.py#L29) in the `data.py` script. We modified this function so it also returns the pretrained embeddings, along with the loaded train data, dev data, test data, and the source and target vocabularies:\n",
    "\n",
    "```python\n",
    " return train_data, dev_data, test_data, src_vocab, pretrained_embed, trg_vocab\n",
    "```\n",
    "\n",
    "2. The source and target vocabularies, as well as the embeddings, are passed to initialize the model. Here is also when the embedding layer is initialized, just as we saw before.\n",
    "\n",
    "\n",
    "3. `SignProdDataset`s are initialized with the training, dev and test data. This class inherits from `torch.data.Dataset` and provides some utilities to handle the data. The data itself is read from the source files, separating lines by newline character, and then tokenizing words with a simple `string.split()`.\n",
    "\n",
    "\n",
    "4. Batches are drawn from the `SignProdDataset`s. The original project implements a `Batch` class in `batch.py` for this. Again, this class includes some useful utilities.\n",
    "\n",
    "\n",
    "5. The batches are passed to the model and the loss is calculated.\n",
    "\n",
    "\n",
    "6. The loss is backpropagated and a new training step starts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b719d04",
   "metadata": {},
   "source": [
    "Looking at the step 3 is when **we realized**: the **words were simply split (tokenized) by whitespace**, but as we have seen before, the **tokenization in WordPiece is a little bit more complex**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ee500",
   "metadata": {},
   "source": [
    "### Properly tokenizing the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809cf555",
   "metadata": {},
   "source": [
    "So how do we properly tokenize the input data then? Luckily, the great Transformers ü§ó library is there to make our lives easier.\n",
    "\n",
    "To tokenize a string with our tokenizer, we simply need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "02f6a540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [102, 233, 773, 2355, 1261, 348, 5654, 806, 990, 5654, 207, 127, 1261, 552, 798, 12103, 148, 608, 22906, 8291, 30939, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "string = \"Es gibt keinen Weg zum Frieden, denn Frieden ist der Weg. ‚Äì Mahatma Gandhi\"\n",
    "\n",
    "print(constants.tokenizer(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8604906e",
   "metadata": {},
   "source": [
    "The only \"problem\" is that we get the input ids, but we can solve this easily with a dictionary lookup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b859367b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]',\n",
      " 'es',\n",
      " 'gibt',\n",
      " 'keinen',\n",
      " 'weg',\n",
      " 'zum',\n",
      " 'frieden',\n",
      " ',',\n",
      " 'denn',\n",
      " 'frieden',\n",
      " 'ist',\n",
      " 'der',\n",
      " 'weg',\n",
      " '.',\n",
      " '‚Äì',\n",
      " 'mah',\n",
      " '##at',\n",
      " '##ma',\n",
      " 'gan',\n",
      " '##dh',\n",
      " '##i',\n",
      " '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "ids = constants.tokenizer(string)['input_ids']\n",
    "pprint([constants.vocab[id_][0] for id_ in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673b4c9",
   "metadata": {},
   "source": [
    "Cool! We can see that the words that are not in the vocabulary are split (e.g., Mahatma Gandhi). This way of back-off can help making sense out of unknown words. \"Mahatma Gandhi\" is not a very good example, but if we try for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7931a68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'speise', '##karte', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "string = \"Speisekarte\"\n",
    "\n",
    "ids = constants.tokenizer(string)['input_ids']\n",
    "pprint([constants.vocab[id_][0] for id_ in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb06ee3",
   "metadata": {},
   "source": [
    "We see that even thoug \"Speisekarte\" is not in the vocabulary \"speise\" and \"karte\" are there, which sometimes can help infer the meaning of the complete word (which in German often times works well)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fa1326",
   "metadata": {},
   "source": [
    "### Using the tokenized the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb006dc5",
   "metadata": {},
   "source": [
    "One last thing we need to do is, when using the pretrained embeddings, to use our tokenization function above instead of `string.split`.\n",
    "\n",
    "For that, we included our function in `data.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "de2deb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenization(string: str):\n",
    "    \"\"\"Tokenize a string using the BERT tokenizer.\"\"\"\n",
    "    # Tokenize and remove [CLS] and [SEP] (first and last tokens),\n",
    "    # since they will be added later.\n",
    "    ids = constants.tokenizer(string)['input_ids'][1:-1]\n",
    "    return [constants.vocab[id_][0] for id_ in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d2efe3",
   "metadata": {},
   "source": [
    "Something that we also take care of is removing the [CLS] and [SEP] tokens, since they will be added afterwards when preparing the batches' examples.\n",
    "\n",
    "\n",
    "Finally, the way we integrate our function in the existing code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "30884968",
   "metadata": {},
   "outputs": [],
   "source": [
    "if constants.pretrained_model_str == \"bert\":  # using pretrained embeddings\n",
    "    tok_fun = bert_tokenization\n",
    "else:\n",
    "    tok_fun = lambda s: list(s) if level == \"char\" else s.split()\n",
    "\n",
    "# Source field is a tokenized version of the source words\n",
    "src_field = data.Field(\n",
    "    init_token=None,\n",
    "    eos_token=constants.EOS_TOKEN,\n",
    "    pad_token=constants.PAD_TOKEN,\n",
    "    tokenize=tok_fun,\n",
    "    batch_first=True,\n",
    "    lower=False,  # data already lowercased\n",
    "    unk_token=constants.UNK_TOKEN,\n",
    "    include_lengths=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae7aff",
   "metadata": {},
   "source": [
    "The [`torchtext.data.Field`](https://torchtext.readthedocs.io/en/latest/data.html#field) class will take care of the tokenization, we simply need to pass it the tokenizaiton function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da33701c",
   "metadata": {},
   "source": [
    "### Now we ARE ready! ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc686f",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba297d8",
   "metadata": {},
   "source": [
    "## #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f019fd8",
   "metadata": {},
   "source": [
    "#  Data Augmentation by backtranslation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa96ac",
   "metadata": {},
   "source": [
    "Data Augmentation describes a set of algorithms that construct synthetic data from an available dataset. This synthetic data typically contains small changes in the data that the model‚Äôs predictions should be invariant to. Synthetic data can also represent combinations between distant examples that would be very difficult to infer otherwise. It's worth noting that data augmentation is a regularising approach, meaning that it tends to reduce model variance by making training harder. \n",
    "\n",
    "In our case, we propose the use of backtranslation technique in order to augment our dataset. This method is presented in various works [[12]](#ref_12), [[13]](#ref_13), [[15]](#ref_15), [[16]](#ref_16).\n",
    "\n",
    "The objective is to generate paraphrasing, introduce synonyms or different grammatical structures into the data distribution to develop invariance to these changes.\n",
    "\n",
    "In order to achieve this we leveraged models from the `hugging-face` `transformers` library.\n",
    "\n",
    "* **German to English translation:**: \n",
    "\n",
    "The model used for this task was based on a transformer-align architecture, which is an architecture that simultaneously learns how to translate and align text. This implementation was done by the NLP Lab at Helsinki University.\n",
    "\n",
    "The dataset used is [opus](https://opus.nlpl.eu/), which is a collection of translated texts from the web. Also the translation pipeline includes text normalization and tokenization with *SentencePiece*, which is an unsupervised text tokenizer (https://github.com/google/sentencepiece).\n",
    "\n",
    "\n",
    "* **English to Germas translation:**: \n",
    "\n",
    "The model used was a pretrained out-of-the-box pipeline from the `transformers` library. Specifically a *Text-to-Text Transfer Transformer*, which is a modification of the renowned BERT model which introduces a couple of architecture tweaks. The variant imlemented in this project is the [T5-base](https://huggingface.co/t5-base), which was pre-trained on an extended version of the well-known [Common Crawl dataset](https://commoncrawl.org).\n",
    "\n",
    "\n",
    "Some examples:\n",
    "\n",
    "*Original*\n",
    "> und nun die wettervorhersage f√ºr morgen donnerstag den zw√∂lften august.\n",
    "\n",
    "*Backtranslated*\n",
    "> und jetzt die wettervorhersage f√ºr morgen donnerstag, den zw√∂lften august\n",
    "\n",
    "*Original*\n",
    "> starke winde sorgen zudem f√ºr schneeverwehungen es bestehen entsprechende unwetterwarnungen des deutschen wetterdienstes\n",
    "\n",
    "*Backtranslated*\n",
    "> starke winde sorgen auch f√ºr schneedriften, es gibt entsprechende wetterwarnungen des deutschen wetterdienstes\n",
    "\n",
    "This process resulted in having double the training samples, as for each *phrase,skeleton* pair we have another *backtranslated,skeleton* pair. We additionally had to perform extra normalization to the generated text to have an equivalent format across all the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682e995",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac426c",
   "metadata": {},
   "source": [
    "<a id='ref_1'>[1]</a> WHO: World Health Organization. Deafness and hearing loss. http://www.who.int/mediacentre/factsheets/fs300/en/, 2021\n",
    "\n",
    "<a id='ref_2'>[2]</a> Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. Multimodal deep hand sign language recognition in still images using restricted boltzmann machine. Entropy, 20, 2018.\n",
    "\n",
    "<a id='ref_3'>[3]</a> Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. Hand sign language recognition using multi-view hand skeleton. Expert Systems With Applications, 150, 2020.\n",
    "\n",
    "<a id='ref_4'>[4]</a> Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. Video based isolated hand sign language recognition using a deep cascaded model. Multimedia Tools And Applications, 79:22965‚Äì22987, 2020.\n",
    "\n",
    "<a id='ref_5'>[5]</a> Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. Hand pose aware multimodal isolated sign language recognition. Multimedia Tools And Applications, 80:127‚Äì163, 2021\n",
    "\n",
    "<a id='ref_6'>[6]</a> Mark Borg and Kenneth P. Camilleri. Phonologically-meaningful sub-units for deep learning-based sign language recognition. ECCV, 2020\n",
    "\n",
    "<a id='ref_7'>[7]</a> Agelos Kratimenos, Georgios Pavlakos, and Petros Maragos. 3d hands, face and body extraction for sign language recognition. ECCV, 2020.\n",
    "\n",
    "<a id='ref_8'>[8]</a> Razieh Rastgoo and Kourosh Kiani and Sergio Escalera and Mohammad Sabokrou. Sign Language Production: A Review. 2021.\n",
    "\n",
    "<a id='ref_9'>[9]</a> Saunders, Ben and Camgoz, Necati Cihan and Bowden, Richard. Progressive Transformers for End-to-End Sign Language Production. https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560664.pdf. ECCV, 2020.\n",
    "\n",
    "<a id='ref_10'>[10]</a> J. Forster, C. Schmidt, T. Hoyoux, O. Koller, U. Zelle, J. Piater, and H. Ney. RWTH-PHOENIX-Weather: A Large Vocabulary Sign Language Recognition and Translation Corpus. https://www-i6.informatik.rwth-aachen.de/publications/download/773/Forster-LREC-2012.pdf In Language Resources and Evaluation (LREC), pages 3785-3789, Istanbul, Turkey, May 2012. \n",
    "\n",
    "<a id='ref_11'>[11]</a> Attention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. https://arxiv.org/pdf/1706.03762.pdf, June 2017.\n",
    "\n",
    "<a id='ref_12'>[12]</a>  BET: A Backtranslation Approach for Easy Data Augmentation in Transformer-based Paraphrase Identification Context. Jean-Philippe Corbeil, Hadi Abdi Ghadivel. https://arxiv.org/abs/2009.12452, Sep 2020\n",
    "\n",
    "<a id='ref_13'>[13]</a> Data augmentation using back-translation for context-aware neural machine translation. Sugiyama & Yoshinaga. https://aclanthology.org/D19-6504, EMNLP 2019)\n",
    "\n",
    "\n",
    "<a id='ref_14'>[14]</a> Handspeak. Negation in Sign Language. https://www.handspeak.com/learn/index.php?id=156, 2022.\n",
    "\n",
    "<a id='ref_15'>[15]</a> Data expansion using back translation and paraphrasing for hate speech detection. Djamila RomaissaBeddiar SaroarJahan, MouradOussalah. https://doi.org/10.1016/j.osnem.2021.100153, November 2019\n",
    "\n",
    "<a id='ref_16'>[16]</a> Text Data Augmentation for Deep Learning. Shorten, C., Khoshgoftaar, T.M. & Furht, B. https://doi.org/10.1186/s40537-021-00492-0, July 2021\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
