{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541e0da8",
   "metadata": {},
   "source": [
    "<img vspace=\"33px\" align=\"right\" src=\"https://www.munich-startup.de/wp-content/uploads/2019/03/TUM_logo-440x236.png\" width=\"120px\"/>\n",
    "<h1>Sign Language Production</h1>\n",
    "<h3>Applied Deep Learning for NLP</h3>\n",
    "<p><b>Diego Miguel Lozano</b> | <b>Wenceslao Villegas Marset</b></p>\n",
    "<p>March 9<sup>th</sup>, 2022</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afad8b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172e26d",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "> ### [1. Introduction](#section_1)\n",
    ">> [**1.1 What is Sign Language Production (SLP)?**](#section_1_1)<br>\n",
    ">> [**1.2 What is the starting point for our project?**](#section_1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca96727",
   "metadata": {},
   "source": [
    "<a id='section_1'></a>\n",
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b2c8e",
   "metadata": {},
   "source": [
    "<a id='section_1_1'></a>\n",
    "### 1.1 What is Sign Language Production (SLP)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37110ff",
   "metadata": {},
   "source": [
    "Sign Language Production focuses on translating spoken languages into sign languages and viceversa. According to the World Health Organization (WHO), in 2020 there were more than 466 million deaf people in the world [[1]](#ref_1). This area could be of great help for the hearing-impared community, being for that necessary the development of techniques for both recognition and production of sign languages.\n",
    "\n",
    "While the Sign Language Recognition has seen numerous advancements in the last years [[2](#ref_2), [3](#ref_3), [4](#ref_4), [5](#ref_5), [6](#ref_6), [7](#ref_7)], Sign Language Production is still a very challenging task, since it involves an interpretation between visual and linguistic information [[8]](#ref_8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438970b4",
   "metadata": {},
   "source": [
    "<a id='section_1_2'></a>\n",
    "### 1.2 What is the starting point for our project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731e567",
   "metadata": {},
   "source": [
    "As we just mentioned, SLP is complex and far from being solved. Nevertheless, there have recently been promising developments, such as the application of Transformer architectures to SLP, what has come to be called \"Progressive Transformers.\"\n",
    "\n",
    "In this project, we take the [source code](https://github.com/BenSaunders27/ProgressiveTransformersSLP) for the paper \"Progressive Transformers for End-to-End Sign Language Production\" [[9]](#ref_9) as the starting point.\n",
    "\n",
    "**TODO**: define exactly what the scope of our project is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd906eba",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62948faf",
   "metadata": {},
   "source": [
    "# A brief intro to the \"Progressive Transformers for SLP\" project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf07b3a",
   "metadata": {},
   "source": [
    "In this section, we will quickly explain the main aspects of Progressive Transformers project. If we had to summarize it in only three points, these would be the following: counter decoding, two different approaches ‚ÄìText-to-Gloss-to-Pose (T2G2P) and Text-to-Pose (T2P)‚Äì, and data augmentation.\n",
    "\n",
    "## Counter decoding\n",
    "\n",
    "One of the main challenges of SLP is that the output has to maintain certain continuity. The predicted pose in a video frame has to flow naturally from the previous one, and analogously for the frames that follow. This is achieved in the following manner: the model not only predicts the sign pose, but also a \"counter\". This counter is nothing else but real number in the interval [0, 1]. This value increases monotonically from 0 to 1.0, marking thus the begining and end of sequence, respectively.\n",
    "\n",
    "<img width=\"600px\" src=\"./images/counter-decoding.jpg\" alt=\"Counter Decoding\"/>\n",
    "<br>\n",
    "<div align=\"center\"><i>Representation of counter decoding.</i></div>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "**‚ÑπÔ∏è Question:** Why not simply use an BOS token and EOS?\n",
    "\n",
    "**üí° Answer:** Begining of Sentence (BOS) and End of Sentence (EOS) tokens work well with sentences, but when producing video as we mentioned before we need something more than just marking the begging and end of it. Therefore, the counter serves both as an BOS and EOS and captures information about the flow of the video.\n",
    "\n",
    "---\n",
    "\n",
    "### Two different approaches\n",
    "\n",
    "In the paper, they experimented with two different approaches: T2G2P and T2P:\n",
    "\n",
    "<img width=\"600px\" src=\"./images/T2G2P-vs-T2P.png\" alt=\"T2G2P vs T2P Architectures\"/>\n",
    "<br>\n",
    "<div align=\"center\"><i>Architecture details of (a) Symbolic and (b) Progressive Transformers. (ST: Symbolic Transformer, PT: Progressive Transformer, PE: Positional Encoding, CE: Counter Embedding, MHA: Multi-Head Attention) <a href=\"#ref_10\">[10]</a>.</i></div>\n",
    "\n",
    "In both cases, the models follow the architecture introduced in \"Transfomers is All You Need\" <a href=\"#ref_11\">[10]</a>.\n",
    "\n",
    "In the first approach ‚ÄìT2G2P‚Äì glosses are produced from the input tokens in a first step. Then, this glosses serve as input for another transformer, which then translates the glosses into sign poses.\n",
    "\n",
    "The second ‚ÄìT2P‚Äì is an end-to-end approach, in which the text is directly translated into sign poses.\n",
    "\n",
    "### Data augmentation\n",
    "\n",
    "Finally, the paper explores some data augmentation techniques to determine whether they improve the base model. These augmentations where only carried out with the T2G2P architecture.\n",
    "\n",
    "- **Future Prediction**: this type of augmentation forces the model to predict the next 10 frames from the current time step, instead of just the next frame. In this way, the model cannot just copy the previous time step, which effectively improves performance over the base architecture.\n",
    "\n",
    "\n",
    "- **Just Counter**: in this case only the counter values are provided as target input to the model, omitting the 3D skeleton joint coordinates. Again, this has shown to improve results.\n",
    "\n",
    "\n",
    "- **Gaussian Noise**: the last augmentation method consists in adding Gaussian noise to the skeleton pose sequences during training. This makes the model more robust to prediction inputs.\n",
    "\n",
    "\n",
    "The following table collects the results of the previous augmentation approaches:\n",
    "\n",
    "<img width=\"700px\" src=\"./images/augmentation-results.png\" alt=\"Data Augmentation Results\"/>\n",
    "<br>\n",
    "<div align=\"center\"><i>The best BLEU-4 performance comes from a combination of future prediction and Gaussian noise augmentation. The model must learn to cope with both multi-frame prediction and a noisy input, building a firm robustness to drift <a href=\"#ref_10\">[10]</a>.</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1796cf9",
   "metadata": {},
   "source": [
    "### #TODO: document main aspects of their code (Wences?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c3a42c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cae32e",
   "metadata": {},
   "source": [
    "#  Using pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c2d9e7",
   "metadata": {},
   "source": [
    "**TODO**: mention that the original project trains embeddings from scratch and that we could leverage pre-trained embeddings (e.g., BERT) to achieve better scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada48393",
   "metadata": {},
   "source": [
    "## Inside the SLP pretrained model vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94977066",
   "metadata": {},
   "source": [
    "The original project that we use as starting point provides a plain-text file [`src_vocab`](https://github.com/BenSaunders27/ProgressiveTransformersSLP/blob/master/Configs/src_vocab.txt) containing the vocabulary for which embeddings will then be trained. Here is a snippet of it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de685e28",
   "metadata": {},
   "source": [
    "```\n",
    "<unk>\n",
    "<pad>\n",
    "<s>\n",
    "</s>\n",
    "...\n",
    "AUSWAEHLEN\n",
    "BALD\n",
    "BEKOMMEN\n",
    "BITTE\n",
    "BODENSEE\n",
    "BRITANNIEN\n",
    "CHAOS\n",
    "DAMEN\n",
    "DAUERND\n",
    "DUENN\n",
    "...\n",
    "IRGENDWO\n",
    "J+L+I\n",
    "K+R+E+T+A\n",
    "...\n",
    "neg-DEUTSCH\n",
    "neg-FUENF\n",
    "neg-GEMUETLICH\n",
    "neg-GENUG\n",
    "neg-GLEICH\n",
    "neg-HART\n",
    "neg-HEISS\n",
    "...\n",
    "negalp-MUSS\n",
    "negalp-PASSEN\n",
    "negalp-STIMMT\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097a094",
   "metadata": {},
   "source": [
    "Before jumping in and trying to directly use our pretrained embeddings, it is sensible to first analyze a bit how things work in the original project.\n",
    "\n",
    "From the previous vocabulary, there are three aspects that are worth mentioning:\n",
    "\n",
    "- Words such as `AUSWAEHLEN`, `DUENN` and `HEISS` give us a hint that **normalization** is used. A popular algorithm for German normalization is the [German2 snowball algorithm](https://snowballstem.org/algorithms/german2/stemmer.html) which defines the following mappings:\n",
    "  - '√ü' is replaced by 'ss'.\n",
    "  - '√§', '√∂', '√º' are replaced by 'a', 'o', 'u', respectively.\n",
    "  - 'ae' and 'oe' are replaced by 'a', and 'o', respectively.\n",
    "  - 'ue' is replaced by 'u', when not following a vowel or q.\n",
    "\n",
    "\n",
    "- As we saw during the seminar lectures, the **special tokens** `<unk>`, `<pad>`, `<s>`, `</s>` are also included in the dictionary. These tokens mark unknown words, padding, beginning of sequence (BOS), and end of sequence (EOS), respectively.\n",
    "\n",
    "\n",
    "- Some of the words in the vocabulary include the prefixes `neg-` and `negalp-`. We could guess that `neg-` simply means that the word is negated, e.g., `neg-GENUG`‚â° `NICHT GENUG`, but what about the `negalp-` prefix? And also, what do words such as `J+L+I` and `K+R+E+T+A` mean? A look to the paper of the RWTH-PHOENIX-Weather dataset [[10]](#ref_10) (the first version of the dataset used to train the model) gives us the answer:\n",
    "\n",
    "\n",
    "<img width=\"400px\" src=\"./images/RWTH-PHOENIX-Weather-Annotation-Scheme.png\" alt=\"RWTH-PHOENIX-Weather Annotation Scheme\"/>\n",
    "<br>\n",
    "<div align=\"center\"><i>Source <a href=\"#ref_10\">[10]</a>.</i></div>\n",
    "\n",
    "So in reality `neg-` means \"signs negated by headshake\" and `negalp-` \"signs negated by the alpha[betical] rule\" <sup>[1](note_1)</sup>. Words such as `K+R+E+T+A` are words (finger) spelled letter by letter.\n",
    "\n",
    "Interestingly enough, none of the other types of tokens appear in the source dictionary.\n",
    "\n",
    "---\n",
    "\n",
    "<a id='note_1'><sup>1</sup></a> In Sign Language, there are several ways of negating words. One of these ways is using a side-to-side headshake or a frown expression. Also, some verbs have their own negated forms, which is what `negalp-` indicates here [[11]](#ref_11)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7103277",
   "metadata": {},
   "source": [
    "### But... where is this vocabulary coming from?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad625ae",
   "metadata": {},
   "source": [
    "As it turns out, this vocabulary is simply made up of **glosses**. As we mentioned before, the original project proposes two ways of carrying out the translations from text to sign language.\n",
    "\n",
    "The first one consists in predicting glosses from text, and the translating the glosses to sign language (more precisely, coordinates).\n",
    "\n",
    "The second approach directly translates from text to sign language, in an end-to-end fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c494d0fc",
   "metadata": {},
   "source": [
    "## BERT it up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53989e",
   "metadata": {},
   "source": [
    "If something is clear is that BERT's dictionary will not contain glosses, let alone glosses specifically tailored to SLP.\n",
    "\n",
    "But then, how does BERT's vocabulary looks like? Let's take a look!\n",
    "\n",
    "Fortunately, Hugging Face's great API has got us covered: tokenizers expose their vocabulary through the method `get_vocab()`. Let's try with the model [`bert-base-german-cased`](https://huggingface.co/bert-base-german-cased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1d982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load tokenizer from pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab3b0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[unused1974]', 28973),\n",
      " ('Andy', 22652),\n",
      " ('Konkur', 4558),\n",
      " ('Ferdinand', 8715),\n",
      " ('Besondere', 21453),\n",
      " ('##ago', 5572),\n",
      " ('01.', 9792),\n",
      " ('Pok', 11441),\n",
      " ('fordert', 8559),\n",
      " ('58', 8393),\n",
      " ('Rezens', 14475),\n",
      " ('klass', 4457),\n",
      " ('√ñsterreich', 2661),\n",
      " ('Anh√§ngern', 23532),\n",
      " ('[unused546]', 27545),\n",
      " ('Beschleun', 21506),\n",
      " ('Kaufpreis', 14774),\n",
      " ('bewirken', 22453),\n",
      " ('##f√§r', 25424),\n",
      " ('Honorar', 14227),\n",
      " ('bestehende', 7726),\n",
      " ('Personal', 3959),\n",
      " ('Verhandlungs', 16663),\n",
      " ('Rese', 14429),\n",
      " ('177', 18927),\n",
      " ('wirkte', 6420),\n",
      " ('schien', 12867),\n",
      " ('ungl√ºck', 21829),\n",
      " ('legitim', 20663),\n",
      " ('[unused1068]', 28067),\n",
      " ('##bek', 6295),\n",
      " ('##fahrts', 13135),\n",
      " ('W√∂rter', 14944),\n",
      " ('Abk', 14423),\n",
      " ('Rechnungen', 17913),\n",
      " ('[unused65]', 27064),\n",
      " ('kurze', 7478),\n",
      " ('[unused343]', 27342),\n",
      " ('Prinzessin', 15653),\n",
      " ('Periode', 21859)]\n"
     ]
    }
   ],
   "source": [
    "# Print some words of its dictionary\n",
    "pprint(list(tokenizer.get_vocab().items())[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c0f42",
   "metadata": {},
   "source": [
    "Above, we can see each token with its corresponding ID (just as we saw in the seminar lectures). However, there are two things that catch our attention...\n",
    "\n",
    "<br>\n",
    "\n",
    "**Why do some tokens start with \"##\"?**\n",
    "\n",
    "Well, this is just a way of indicating that this token is \"non-initial\", i.e., originally it belonged to a longer word (remember that usually Transformers work at a sub-word level).\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**What about the `[unused###]` tokens?**\n",
    "\n",
    "These are, unsurprisingly, tokens that are not used. However, they can come handy to add more words to the vocabulary:\n",
    "\n",
    "> Just replace the \"[unusedX]\" tokens with your vocabulary. Since these were not used they are effectively randomly initialized. ([source](https://github.com/google-research/bert/issues/9#issuecomment-434796704))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34450b1",
   "metadata": {},
   "source": [
    "As a side note, we would like to mention that someone took the time to explore BERT's vocabulary and wrote a great article about it. The article can be found at https://juditacs.github.io/2019/02/19/bert-tokenization-stats.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac9238",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81caeb7",
   "metadata": {},
   "source": [
    "<a id='ref_1'>[1]</a> WHO: World Health Organization. Deafness and hearing loss. http://www.who.int/mediacentre/factsheets/fs300/en/, 2021\n",
    "\n",
    "<a id='ref_2'>[2]</a> Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. Multimodal deep hand sign language recognition in still images using restricted boltzmann machine. Entropy, 20, 2018.\n",
    "\n",
    "<a id='ref_3'>[3]</a> Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. Hand sign language recognition using multi-view hand skeleton. Expert Systems With Applications, 150, 2020.\n",
    "\n",
    "<a id='ref_4'>[4]</a> Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. Video based isolated hand sign language recognition using a deep cascaded model. Multimedia Tools And Applications, 79:22965‚Äì22987, 2020.\n",
    "\n",
    "<a id='ref_5'>[5]</a> Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. Hand pose aware multimodal isolated sign language recognition. Multimedia Tools And Applications, 80:127‚Äì163, 2021\n",
    "\n",
    "<a id='ref_6'>[6]</a> Mark Borg and Kenneth P. Camilleri. Phonologically-meaningful sub-units for deep learning-based sign language recognition. ECCV, 2020\n",
    "\n",
    "<a id='ref_7'>[7]</a> Agelos Kratimenos, Georgios Pavlakos, and Petros Maragos. 3d hands, face and body extraction for sign language recognition. ECCV, 2020.\n",
    "\n",
    "<a id='ref_8'>[8]</a> Razieh Rastgoo and Kourosh Kiani and Sergio Escalera and Mohammad Sabokrou. Sign Language Production: A Review. 2021.\n",
    "\n",
    "<a id='ref_9'>[9]</a> Saunders, Ben and Camgoz, Necati Cihan and Bowden, Richard. Progressive Transformers for End-to-End Sign Language Production. https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560664.pdf. ECCV, 2020.\n",
    "\n",
    "<a id='ref_10'>[10]</a> J. Forster, C. Schmidt, T. Hoyoux, O. Koller, U. Zelle, J. Piater, and H. Ney. RWTH-PHOENIX-Weather: A Large Vocabulary Sign Language Recognition and Translation Corpus. https://www-i6.informatik.rwth-aachen.de/publications/download/773/Forster-LREC-2012.pdf In Language Resources and Evaluation (LREC), pages 3785-3789, Istanbul, Turkey, May 2012. \n",
    "\n",
    "<a id='ref_11'>[11]</a> Attention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. https://arxiv.org/pdf/1706.03762.pdf, June 2017.\n",
    "\n",
    "<a id='ref_11'>[12]</a> \n",
    "\n",
    "<a id='ref_11'>[13]</a> \n",
    "\n",
    "<a id='ref_11'>[14]</a> Handspeak. Negation in Sign Language. https://www.handspeak.com/learn/index.php?id=156, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8a156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
