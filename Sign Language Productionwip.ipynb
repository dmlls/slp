{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6abe7d",
   "metadata": {},
   "source": [
    "<img vspace=\"33px\" align=\"right\" src=\"https://www.munich-startup.de/wp-content/uploads/2019/03/TUM_logo-440x236.png\" width=\"120px\"/>\n",
    "<h1>Sign Language Production</h1>\n",
    "<h3>Applied Deep Learning for NLP</h3>\n",
    "<p><b>Diego Miguel Lozano</b> | <b>Wenceslao Villegas Marset</b></p>\n",
    "<p>March 9<sup>th</sup>, 2022</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe5979",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23da87",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "> ### [1. Introduction](#section_1)\n",
    ">> [**1.1 What is Sign Language Production (SLP)?**](#section_1_1)<br>\n",
    ">> [**1.2 What is the starting point for our project?**](#section_1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f38d01",
   "metadata": {},
   "source": [
    "<a id='section_1'></a>\n",
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ab10a",
   "metadata": {},
   "source": [
    "<a id='section_1_1'></a>\n",
    "### 1.1 What is Sign Language Production (SLP)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2061012",
   "metadata": {},
   "source": [
    "Sign Language Production focuses on translating spoken languages into sign languages and viceversa. According to the World Health Organization (WHO), in 2020 there were more than 466 million deaf people in the world [[1]](#ref_1). This area could be of great help for the hearing-impared community, being for that necessary the development of techniques for both recognition and production of sign languages.\n",
    "\n",
    "While the Sign Language Recognition has seen numerous advancements in the last years [[2](#ref_2), [3](#ref_3), [4](#ref_4), [5](#ref_5), [6](#ref_6), [7](#ref_7)], Sign Language Production is still a very challenging task, since it involves an interpretation between visual and linguistic information [[8]](#ref_8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2022d6f1",
   "metadata": {},
   "source": [
    "<a id='section_1_2'></a>\n",
    "### 1.2 What is the starting point for our project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3128eb0",
   "metadata": {},
   "source": [
    "As we just mentioned, SLP is complex and far from being solved. Nevertheless, there have recently been promising developments, such as the application of Transformer architectures to SLP, what has come to be called \"Progressive Transformers.\"\n",
    "\n",
    "In this project, we take the [source code](https://github.com/BenSaunders27/ProgressiveTransformersSLP) for the paper \"Progressive Transformers for End-to-End Sign Language Production\" [[9]](#ref_9) as the starting point.\n",
    "\n",
    "We propose to test different improvement approaches to boost the model's performance like:\n",
    "\n",
    "* Using pre-trained BERT embeddings and fine-tuning them during training.\n",
    "* Leveraging different pre-trained models from the hugging-face ecosystem to perform data augmentation on the source senteces. \n",
    "* Testing out the improvement in performance with different hyperparameter configurations for the transformer architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a088674",
   "metadata": {},
   "source": [
    "<a id='section_1_3'></a>\n",
    "### 1.3 The data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4798e",
   "metadata": {},
   "source": [
    "Source data stems from the RWTH-PHOENIX-Weather-2014T dataset.\n",
    "\n",
    "*Dataset Infomation*: Over a period of three years (2009 - 2011) the daily news and weather forecast airings of the German public tv-station PHOENIX featuring sign language interpretation have been recorded and the weather forecasts of a subset of 386 editions have been transcribed using gloss notation. Furthermore, we used automatic speech recognition with manual cleaning to transcribe the original German speech. As such, this corpus allows to train end-to-end sign language translation systems from sign language video input to spoken language.\n",
    "\n",
    "The signing is recorded by a stationary color camera placed in front of the sign language interpreters. Interpreters wear dark clothes in front of an artificial grey background with color transition. All recorded videos are at 25 frames per second and the size of the frames is 210 by 260 pixels. Each frame shows the interpreter box only.\n",
    "\n",
    "* **Text data.**:\n",
    "\n",
    "Consists of sequences of text that represent the transcription of each video recording.\n",
    "\n",
    "Example: \n",
    "> das bedeutet viele wolken und immer wieder zum teil kr√§ftige schauer und gewitter \n",
    "\n",
    "* **Gloss representation**:\n",
    "\n",
    "Gloss equivalent for the text transcript. A gloss is a German word or words that are used to name the corresponding Sign Language signs.\n",
    "\n",
    "Example: \n",
    "> ES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
    "\n",
    "* **Skeleton**:\n",
    "\n",
    "Sequence of 3D skeletal poses that the model has to learn to produce (ground truth). The format of this data is as follows:\n",
    "\n",
    "* Tuple format: (index of a start point, index of an end point, index of a bone)\n",
    "\n",
    "                (0)\n",
    "                 |\n",
    "                 |\n",
    "                 0\n",
    "                 |\n",
    "                 |\n",
    "        (2)--1--(1)--1--(3)\n",
    "         |               |\n",
    "         |               |\n",
    "         2               2\n",
    "         |               |\n",
    "         |               |\n",
    "        (4)             (5)\n",
    "\n",
    "      has this structure:\n",
    "\n",
    "      (\n",
    "        (0, 1, 0),\n",
    "        (1, 2, 1),\n",
    "        (1, 3, 1),\n",
    "        (2, 4, 2),\n",
    "        (3, 5, 2),\n",
    "      )\n",
    "\n",
    "Then a resulting skeleton pose on a frame would be composed of 150 values, since we have 25 joints with xyz coordinates. And each text sample would have N corresponding frames that would represent the corresponding sequence of skeleton poses. During training the shape of the network output is 151 since a counter is added for the network to predict (explained further down).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4157bb9e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66347de",
   "metadata": {},
   "source": [
    "# A brief intro to the \"Progressive Transformers for SLP\" project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194c2c27",
   "metadata": {},
   "source": [
    "In this section, we will quickly explain the main aspects of Progressive Transformers project. If we had to summarize it in only three points, these would be the following: counter decoding, two different approaches ‚ÄìText-to-Gloss-to-Pose (T2G2P) and Text-to-Pose (T2P)‚Äì, and data augmentation.\n",
    "\n",
    "## Counter decoding\n",
    "\n",
    "One of the main challenges of SLP is that the output has to maintain certain continuity. The predicted pose in a video frame has to flow naturally from the previous one, and analogously for the frames that follow. This is achieved in the following manner: the model not only predicts the sign pose, but also a \"counter\". This counter is nothing else but real number in the interval [0, 1]. This value increases monotonically from 0 to 1.0, marking thus the begining and end of sequence, respectively.\n",
    "\n",
    "<img width=\"600px\" src=\"./images/counter-decoding.jpg\" alt=\"Counter Decoding\"/>\n",
    "<br>\n",
    "<div align=\"center\"><i>Representation of counter decoding.</i></div>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "**‚ÑπÔ∏è Question: Why not simply use an BOS token and EOS?**\n",
    "\n",
    "**üí° Answer:** Begining of Sentence (BOS) and End of Sentence (EOS) tokens work well with sentences, but when producing video as we mentioned before we need something more than just marking the beginning and end of it. Therefore, the counter serves both as an BOS and EOS and captures information about the flow of the video.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Two different approaches\n",
    "\n",
    "In the paper, they experimented with two different approaches: T2G2P and T2P:\n",
    "\n",
    "<img width=\"600px\" src=\"./images/T2G2P-vs-T2P.png\" alt=\"T2G2P vs T2P Architectures\"/>\n",
    "<br>\n",
    "<div align=\"center\"><i>Architecture details of (a) Symbolic and (b) Progressive Transformers. (ST: Symbolic Transformer, PT: Progressive Transformer, PE: Positional Encoding, CE: Counter Embedding, MHA: Multi-Head Attention) <a href=\"#ref_10\">[10]</a>.</i></div>\n",
    "\n",
    "In both cases, the models follow the architecture introduced in \"Attention is All You Need\" <a href=\"#ref_11\">[10]</a>.\n",
    "\n",
    "In the first approach ‚ÄìT2G2P‚Äì glosses are produced from the input tokens in a first step. Then, this glosses serve as input for another transformer, which then translates the glosses into sign poses.\n",
    "\n",
    "The second ‚ÄìT2P‚Äì is an end-to-end approach, in which the text is directly translated into sign poses.\n",
    "\n",
    "### Data augmentation\n",
    "\n",
    "Finally, the paper explores some data augmentation techniques to determine whether they improve the base model. These augmentations where only carried out with the T2G2P architecture.\n",
    "\n",
    "- **Future Prediction**: this type of augmentation forces the model to predict the next 10 frames from the current time step, instead of just the next frame. In this way, the model cannot just copy the previous time step, which effectively improves performance over the base architecture.\n",
    "\n",
    "\n",
    "- **Just Counter**: in this case only the counter values are provided as target input to the model, omitting the 3D skeleton joint coordinates. Again, this has shown to improve results.\n",
    "\n",
    "\n",
    "- **Gaussian Noise**: the last augmentation method consists in adding Gaussian noise to the skeleton pose sequences during training. This makes the model more robust to prediction inputs.\n",
    "\n",
    "\n",
    "The following table collects the results of the previous augmentation approaches:\n",
    "\n",
    "<img width=\"700px\" src=\"./images/augmentation-results.png\" alt=\"Data Augmentation Results\"/>\n",
    "<br>\n",
    "<div align=\"center\"><i>The best BLEU-4 performance comes from a combination of future prediction and Gaussian noise augmentation. The model must learn to cope with both multi-frame prediction and a noisy input, building a firm robustness to drift <a href=\"#ref_10\">[10]</a>.</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d98915",
   "metadata": {},
   "source": [
    "### Implementation description.\n",
    "\n",
    "The model architecture is based on the \"Attention is All You Need\" <a href=\"#ref_11\">[10]</a> paper. \n",
    "\n",
    "A relevant modification was introduced to adapt it and achieve good performance on the SLP task. \n",
    "* The final layer for the encoder is a Linear one with  512 + 1 units, which represent *coordinates of the output skeleton* + *counter decoding value*.\n",
    "\n",
    "\n",
    "\n",
    "### Overall project structure.\n",
    "\n",
    "Below a descripption of the modules present in the project after being further extended by us.\n",
    "\n",
    "```\n",
    "slp\n",
    "‚îÇ   README.md\n",
    "‚îÇ   Sign Language Production.ipynb    \n",
    "‚îî‚îÄ‚îÄ‚îÄimages\n",
    "‚îî‚îÄ‚îÄ‚îÄProgressiveTransformersSLP\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄConfigs\n",
    "‚îÇ       ‚îÇ   Base.yaml - Config file to set model, data loading/processing and training parameters.\n",
    "‚îÇ       ‚îÇ   src_vocab.txt - GLOSS vocabulary.\n",
    "‚îÇ       ‚îÇ   ...\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄData\n",
    "‚îÇ       ‚îÇ   train.text - Speech text for each training sample.\n",
    "‚îÇ       ‚îÇ   train.skels - Skeleton annotaions for each training sample.\n",
    "‚îÇ       ‚îÇ   train.gloss - Glosses for each training sample.\n",
    "‚îÇ       ‚îÇ   ...\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄexternal_metrics\n",
    "‚îÇ       ‚îÇ   mscoco_rouge.py - ROUGE-L metric implementation.\n",
    "‚îÇ       ‚îÇ   train.gloss - BLEU metric implementation like in https://github.com/mjpost/sacrebleu\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄoptim - Some implementations of optimization algorithms such as: lamb, RAdam, Ranger, etc.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄ__main__.py - Main entrypoing to run model training.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄbatch.py - Wrapper over torch batch iterator, adding masking and other attributes.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄbuilders.py - Assorted builder functions.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄconstants.py - Project wide constants.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄdata.py - Data loading utilities and main torchtext.data.Dataset class\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄdecoders.py - Transformer decoder implementation.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄdtw.py - Dynamic time warping imlementation as in https://github.com/pierre-rouanet/dtw.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄdecoders.py - Transformer decoder implementation.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄembedding.py - Embedding class implementation with support for BERT pretrained ones.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄencoders.py - Transformer encoder implementation.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄhelpers.py - Helper functions for logging, reporting, etc.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄinitialization.py - Custom NN parameter initialization functions.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄloss.py - Loss function implementations.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄmetrics.py - Performance metric functions.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄmodel.py - Main class assembling all the model's modules (decoder/encoder) and constructor functions.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄplot_videos.py - Validataion video generation for skeleton predictions.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄprediction.py - Code for running validation steps on dev set data (perform dtw then loss, etc).\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄsearch.py - Greedy hyperparameter search function.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄtraining.py - Training loop implementation.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄtransformer_layers.py - Layer implementations from NMT toolkit.\n",
    "|       ‚îî‚îÄ‚îÄ‚îÄvocabulary.py - Vocabulary loading and checking code.\n",
    "‚îî‚îÄ‚îÄ‚îÄaugmentations\n",
    "    ‚îÇ   backtranslation.py - Code to perform DE -> EN -> DE translation for augmentation purposes.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbd81f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d79eb",
   "metadata": {},
   "source": [
    "#  Using pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc2522",
   "metadata": {},
   "source": [
    "The original project trains embeddings from scratch. As we have learned during the seminar, the use of pretrained embeddings can effectively improve the performance of models, especially in situations where data is scarce (which is our case).\n",
    "\n",
    "Let's first see how the embedding initialization is happening in the original source code.\n",
    "\n",
    "The first thing that happens when beginning the training is the data loading. The vocabulary of the model is initalized differently depending if we are using T2G2P or T2P.\n",
    "\n",
    "- **Bulding vocabulary in T2G2P**: in this case, the vocabulary is taken from a file [`src_vocab`](https://github.com/BenSaunders27/ProgressiveTransformersSLP/blob/master/Configs/src_vocab.txt) that we will analyze a bit more in depth later.\n",
    "\n",
    "\n",
    "- **Bulding vocabulary in T2P**: when using the End-to-End approach (Text-to-Pose), the vocabulary is built from the training input data.\n",
    "\n",
    "<br>\n",
    "\n",
    "In both cases, the function [`build_vocab()`](https://github.com/BenSaunders27/ProgressiveTransformersSLP/blob/adbd3e9ea9f1b20063d84021a0d6eb9a124ebb87/vocabulary.py#L130-L187) is used. Let's take a look at it.\n",
    "\n",
    "\n",
    "```python\n",
    "if vocab_file is not None:\n",
    "    # load it from file\n",
    "    vocab = Vocabulary(file=vocab_file)\n",
    "else:\n",
    "    ...\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "First of all, if we have a vocabulary file (like in the case of T2G2P), we initialize the vocabulary from it, as we already mentioned.\n",
    "\n",
    "\n",
    "```python\n",
    "def _from_file(self, file: str) -> None:\n",
    "        \"\"\"\n",
    "        Make vocabulary from contents of file.\n",
    "        File format: token with index i is in line i.\n",
    "\n",
    "        :param file: path to file where the vocabulary is loaded from\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        with open(file, \"r\") as open_file:\n",
    "            for line in open_file:\n",
    "                tokens.append(line.strip(\"\\n\"))\n",
    "        self._from_list(tokens)\n",
    "```\n",
    "\n",
    "This function simply reads the vocabulary file line by line. Since each line contains only one token, there is no further processing to be done.\n",
    "\n",
    "If there is no input vocabulary file, the tokens are extracted from the training dataset.\n",
    "\n",
    "Let's run some code to better visualize this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52996d50",
   "metadata": {},
   "source": [
    "## Inside the original SLP model vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c610d",
   "metadata": {},
   "source": [
    "As we have already mentioned, the original project that we use as starting point provides a plain-text file [`src_vocab`](https://github.com/BenSaunders27/ProgressiveTransformersSLP/blob/master/Configs/src_vocab.txt) containing the vocabulary for which embeddings will then be trained.\n",
    "\n",
    "Before jumping in and trying to directly use our pretrained embeddings, it is sensible to first analyze a bit how things work in the original project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733518ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = Path(\"./ProgressiveTransformersSLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f5d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "sys.path.insert(0, code_dir)  # just so that imports can be resolved\n",
    "\n",
    "from ProgressiveTransformersSLP.vocabulary import Vocabulary, build_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f76d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Path to the vocabulary file\n",
    "vocab_file = code_dir/Path(\"Configs/src_vocab.txt\")\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary = Vocabulary(file=vocab_file)\n",
    "\n",
    "# Get all the tokens in the built vocabulary\n",
    "tokens = [token for token in vocabulary.itos]\n",
    "\n",
    "# We will select some tokens that are worth analyzing\n",
    "selected_tokens = (tokens[1:5] + [tokens[172]] + tokens[531:541] +\n",
    "                   tokens[868:870] + tokens[718:725] + tokens[1085:1088])\n",
    "pprint(selected_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01866a0b",
   "metadata": {},
   "source": [
    "From the previous vocabulary, there are three aspects that are worth mentioning:\n",
    "\n",
    "- Words such as `AUSWAEHLEN`, `DUENN` and `HEISS` give us a hint that **normalization** is used. A popular algorithm for German normalization is the [German2 snowball algorithm](https://snowballstem.org/algorithms/german2/stemmer.html) which defines the following mappings:\n",
    "  - '√ü' is replaced by 'ss'.\n",
    "  - '√§', '√∂', '√º' are replaced by 'a', 'o', 'u', respectively.\n",
    "  - 'ae' and 'oe' are replaced by 'a', and 'o', respectively.\n",
    "  - 'ue' is replaced by 'u', when not following a vowel or q.\n",
    "\n",
    "\n",
    "- As we saw during the seminar lectures, the **special tokens** `<unk>`, `<pad>`, `<s>`, `</s>` are also included in the dictionary. These tokens mark unknown words, padding, beginning of sequence (BOS), and end of sequence (EOS), respectively.\n",
    "\n",
    "\n",
    "- Some of the words in the vocabulary include the prefixes `neg-` and `negalp-`. We could guess that `neg-` simply means that the word is negated, e.g., `neg-GENUG`‚â° `NICHT GENUG`, but what about the `negalp-` prefix? And also, what do words such as `J+L+I` and `K+R+E+T+A` mean? A look to the paper of the RWTH-PHOENIX-Weather dataset [[10]](#ref_10) (the first version of the dataset used to train the model) gives us the answer:\n",
    "\n",
    "\n",
    "<img width=\"400px\" src=\"./images/RWTH-PHOENIX-Weather-Annotation-Scheme.png\" alt=\"RWTH-PHOENIX-Weather Annotation Scheme\"/>\n",
    "<br>\n",
    "<div align=\"center\"><i>Source <a href=\"#ref_10\">[10]</a>.</i></div>\n",
    "\n",
    "So in reality `neg-` means \"signs negated by headshake\" and `negalp-` \"signs negated by the alpha[betical] rule\" <sup>[1](note_1)</sup>. Words such as `K+R+E+T+A` are words (finger) spelled letter by letter.\n",
    "\n",
    "Interestingly enough, none of the other types of tokens appear in the source dictionary.\n",
    "\n",
    "---\n",
    "\n",
    "<a id='note_1'><sup>1</sup></a> In Sign Language, there are several ways of negating words. One of these ways is using a side-to-side headshake or a frown expression. Also, some verbs have their own negated forms, which is what `negalp-` indicates here [[11]](#ref_11)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51200a59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**‚ÑπÔ∏è Question: But... where is this vocabulary coming from?**\n",
    "\n",
    "**üí° Answer:** As it turns out, this vocabulary is simply made up of **glosses**. As we mentioned before, the original project proposes two ways of carrying out the translations from text to sign language. That also explains why when going for the T2P approach we don't use this file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c0e3d",
   "metadata": {},
   "source": [
    "## BERT it up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa8f4e",
   "metadata": {},
   "source": [
    "If something is clear is that BERT's dictionary will not contain glosses, let alone glosses specifically tailored to SLP.\n",
    "\n",
    "But then, how does BERT's vocabulary looks like? Let's take a look!\n",
    "\n",
    "Fortunately, Hugging Face's great API has got us covered: tokenizers expose their vocabulary through the method `get_vocab()`. Let's try with the model [`bert-base-german-cased`](https://huggingface.co/bert-base-german-cased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load tokenizer from pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f95195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some words of its dictionary\n",
    "pprint(list(tokenizer.get_vocab().items())[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b49e6b",
   "metadata": {},
   "source": [
    "Above, we can see each token with its corresponding ID (just as we saw in the seminar lectures). However, there are two things that catch our attention...\n",
    "\n",
    "---\n",
    "\n",
    "**‚ÑπÔ∏è Question: Why do some tokens start with \"##\"?**\n",
    "\n",
    "**üí° Answer:** Well, this is just a way of indicating that this token is \"non-initial\", i.e., originally it belonged to a longer word (remember that usually Transformers work at a sub-word level).\n",
    "\n",
    "---\n",
    "\n",
    "**‚ÑπÔ∏è Question: What about the `[unused###]` tokens?**\n",
    "\n",
    "**üí° Answer:** These are, unsurprisingly, tokens that are not used. However, they can come handy to add more words to the vocabulary:\n",
    "\n",
    "> Just replace the \"[unusedX]\" tokens with your vocabulary. Since these were not used they are effectively randomly initialized. ([source](https://github.com/google-research/bert/issues/9#issuecomment-434796704))\n",
    "\n",
    "---\n",
    "\n",
    "As a side note, we would like to mention that someone took the time to explore BERT's vocabulary and wrote a great article about it. The article can be found at https://juditacs.github.io/2019/02/19/bert-tokenization-stats.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cede522",
   "metadata": {},
   "source": [
    "#  Data Augmentation by backtranslation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32440297",
   "metadata": {},
   "source": [
    "Data Augmentation describes a set of algorithms that construct synthetic data from an available dataset. This synthetic data typically contains small changes in the data that the model‚Äôs predictions should be invariant to. Synthetic data can also represent combinations between distant examples that would be very difficult to infer otherwise. It's worth noting that data augmentation is a regularising approach, meaning that it tends to reduce model variance by making training harder. \n",
    "\n",
    "In our case, we propose the use of backtranslation technique in order to augment our dataset. This method is presented in various works [[12]](#ref_12), [[13]](#ref_13), [[15]](#ref_15), [[16]](#ref_16).\n",
    "\n",
    "In order to achieve this we leveraged models from the `hugging-face` `transformers` library.\n",
    "\n",
    "* **German to English translation:**: \n",
    "\n",
    "\n",
    "\n",
    "* **English to Germas translation:**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682e995",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac426c",
   "metadata": {},
   "source": [
    "<a id='ref_1'>[1]</a> WHO: World Health Organization. Deafness and hearing loss. http://www.who.int/mediacentre/factsheets/fs300/en/, 2021\n",
    "\n",
    "<a id='ref_2'>[2]</a> Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. Multimodal deep hand sign language recognition in still images using restricted boltzmann machine. Entropy, 20, 2018.\n",
    "\n",
    "<a id='ref_3'>[3]</a> Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. Hand sign language recognition using multi-view hand skeleton. Expert Systems With Applications, 150, 2020.\n",
    "\n",
    "<a id='ref_4'>[4]</a> Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. Video based isolated hand sign language recognition using a deep cascaded model. Multimedia Tools And Applications, 79:22965‚Äì22987, 2020.\n",
    "\n",
    "<a id='ref_5'>[5]</a> Razieh Rastgoo, Kourosh Kiani, and Sergio Escalera. Hand pose aware multimodal isolated sign language recognition. Multimedia Tools And Applications, 80:127‚Äì163, 2021\n",
    "\n",
    "<a id='ref_6'>[6]</a> Mark Borg and Kenneth P. Camilleri. Phonologically-meaningful sub-units for deep learning-based sign language recognition. ECCV, 2020\n",
    "\n",
    "<a id='ref_7'>[7]</a> Agelos Kratimenos, Georgios Pavlakos, and Petros Maragos. 3d hands, face and body extraction for sign language recognition. ECCV, 2020.\n",
    "\n",
    "<a id='ref_8'>[8]</a> Razieh Rastgoo and Kourosh Kiani and Sergio Escalera and Mohammad Sabokrou. Sign Language Production: A Review. 2021.\n",
    "\n",
    "<a id='ref_9'>[9]</a> Saunders, Ben and Camgoz, Necati Cihan and Bowden, Richard. Progressive Transformers for End-to-End Sign Language Production. https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560664.pdf. ECCV, 2020.\n",
    "\n",
    "<a id='ref_10'>[10]</a> J. Forster, C. Schmidt, T. Hoyoux, O. Koller, U. Zelle, J. Piater, and H. Ney. RWTH-PHOENIX-Weather: A Large Vocabulary Sign Language Recognition and Translation Corpus. https://www-i6.informatik.rwth-aachen.de/publications/download/773/Forster-LREC-2012.pdf In Language Resources and Evaluation (LREC), pages 3785-3789, Istanbul, Turkey, May 2012. \n",
    "\n",
    "<a id='ref_11'>[11]</a> Attention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. https://arxiv.org/pdf/1706.03762.pdf, June 2017.\n",
    "\n",
    "<a id='ref_12'>[12]</a>  BET: A Backtranslation Approach for Easy Data Augmentation in Transformer-based Paraphrase Identification Context. Jean-Philippe Corbeil, Hadi Abdi Ghadivel. https://arxiv.org/abs/2009.12452, Sep 2020\n",
    "\n",
    "<a id='ref_13'>[13]</a> Data augmentation using back-translation for context-aware neural machine translation. Sugiyama & Yoshinaga. https://aclanthology.org/D19-6504, EMNLP 2019)\n",
    "\n",
    "\n",
    "<a id='ref_14'>[14]</a> Handspeak. Negation in Sign Language. https://www.handspeak.com/learn/index.php?id=156, 2022.\n",
    "\n",
    "<a id='ref_15'>[15]</a> Data expansion using back translation and paraphrasing for hate speech detection. Djamila RomaissaBeddiar SaroarJahan, MouradOussalah. https://doi.org/10.1016/j.osnem.2021.100153, November 2019\n",
    "\n",
    "<a id='ref_16'>[16]</a> Text Data Augmentation for Deep Learning. Shorten, C., Khoshgoftaar, T.M. & Furht, B. https://doi.org/10.1186/s40537-021-00492-0, July 2021\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be557b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
